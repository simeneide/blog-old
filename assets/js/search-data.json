{
  
    
        "post0": {
            "title": "Bayesian Neural Nets and how to train them",
            "content": "Why bayesian neural nets . Robustness: Bayesian neural nets should, at least in theory and there has been some evidence for it, be more robust against out of sample data. | Bayesian framework can incorporate prior information into the models. | Bayesian models can present their uncertainty about a specific outcome, i.e. they can tell when they are uncertain! | . References . This post have used the following references extensively: . Nemeth, C., &amp; Fearnhead, P. (2019). Stochastic gradient Markov chain Monte Carlo, 1–31. | Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubon, D. B. (n.d.). Bayesian Data Analysis Third Edition. | Zhang, R., Li, C., Zhang, J., Chen, C., &amp; Wilson, A. G. (2019). Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning, (2017) | . General setup . We have a dataset $D = [ {x_i, y_i }, i=1,...,n]$, and have defined a likelihood function $P(D| theta)$ where $ theta in R^d$ are some parameters defined in the model. We have also already defined a prior for these parameters $P( theta)$. The posterior distribution can then be written as . begin{equation} P( theta | D) = frac{P(D| theta) * P( theta)}{P(D)} propto P(D| theta) * P( theta) end{equation}Finding $P(D)$ is impossible for all but the simplest cases, and as we are interested in complicated likelihood functions such as neural networks, it will clearly not work for us. Therefore, we only have the posterior distribution up to a constant. Luckily, there has been developed multiple techniques that can find an approximation to the posterior distribution that only requires There exist multiple techniques to infer the posterior distribution of a bayesian neural network: . Variational Inference | Dropout | SWAG | Markov Chain Monte Carlo | Stochastic Markov Chain Monte Carlo (SG-MCMC) | . We will focus on the last two here. . Motivating example . Throughout the post we will use a motivating problem to show what we mean. Assume we have collected $N$ data points $(x_n, y_n), n le N$ where $x_n in R^2$ is a two dimensional vector, and $y_n in R^1$ is a one dimensional response. We generate the data using a very simple &quot;neural net&quot;: $$y_n = beta_0 + beta x_n + N(0, sigma)$$ . where $ beta_0$, $ beta$ and $ sigma$ are the parameters of the model. We will collect all three parameters in the parameter $ theta := { beta_0, beta, sigma }$ just to make it easier to write. . We can see that given both the input data $x_n$ and the parameters $ theta$ y_n is actually normally distributed. That is what we call the likelihood function: . $$ P(D| theta) = prod_{n=1}^N P(y_n | theta, x_n) = prod_{n=1}^N N( beta_0 + beta x_n , sigma)$$ . Let us generate a dataset with $ beta_0 = -1$, $ beta = 2.0$ and $ sigma = 0.1$: . N = 30 beta0 = torch.tensor([-1.0]) beta_true = torch.tensor([2.0]) sigma_true = torch.tensor([0.5]) X = dist.Uniform(-1,1).sample((N,)) Y = beta0 + X *beta_true + sigma_true*torch.normal(0,1, (N,)) data = {&#39;x&#39; : X,&#39;y&#39; : Y} ( alt.Chart(pd.DataFrame(data)).mark_circle(size=60) .encode( x=&#39;x&#39;, y=&#39;y&#39;) .properties(title=&quot;X versus Y in the real data&quot;) #.interactive() ) . Now, the goal in supervised learning is to find an estimate of the parameters $ theta$. Our objective is to maximize the likelihood function plus a prior belief of what the parameters could be. In this example we&#39;ll make a really stupid belief and say that it is equally possible to find the parameters anywhere on the real line: $P( theta) sim 1$. This simplifies the analysis, and we can then write the posterior as proportional to the likelihood only: . $$ P( theta | X,Y) sim prod_{n=1}^N N(y_n | beta_0 + beta x_n , sigma^2) $$We can implement this model in pytorch in the following way: . class BayesNet(nn.Module): def __init__(self, seed = 42): super().__init__() torch.random.manual_seed(seed) # Initialize the parameters with some random values self.beta0 = nn.Parameter(torch.randn((1,))) self.beta = nn.Parameter(torch.randn((1,))) self.sigma = nn.Parameter(torch.tensor([1.0]) ) # this has to be positive def forward(self, data: dict): return self.beta0 + self.beta*data[&#39;x&#39;] def loglik(self, data): # Evaluates the log of the likelihood given a set of X and Y yhat = self.forward(data) logprob = dist.Normal(yhat, self.sigma.abs()).log_prob(data[&#39;y&#39;]) return logprob.sum() def logprior(self): # Return a scalar of 0 since we have uniform prior return torch.tensor(0.0) def logpost(self, data): return self.loglik(data) + self.logprior() . model = BayesNet(seed =42) model.loglik(data) . tensor(-68.5243, grad_fn=&lt;SumBackward0&gt;) . Markov Chain Monte Carlo (MCMC) . MCMC is the &quot;traditional&quot; way of finding the posterior distribution. It is an iterative algorithm (the markov chain name) that is updating the belief of what the model parameters $ theta$ should be. I.e. we end up with a loong chain of parameter values: $ theta_1, theta_2, ..., theta_{n}$. However, we are not looking for a fixed value, but a distribution. Luckily, there are theory that tells us that if you run this sequence for long enough, then all values $ theta_n$ will be samples from the posterior distribution. Voila! . So how does it work? . Metropolis MCMC algorithm . The metropolis MCMC algorithms does nothing but starting in some random position $ theta_0$ and randomly tries to go in a direction (e.g. add a Normal distribution $ theta_1 = theta_0 + N(0,1)$). If we improved (meaning that $P( theta | D)$ increased) we use that new value. If it didnt improve, we may still move to that point depending on some probability. . Formally the algorithm looks like this: . Initialize a starting point $ theta_0$. This can either be random or somewhere you think is reasonable. For instance, you can start in the center of your prior distribution $argmax_{ theta} P( theta)$. | For each $n=1,2,3,...$ do the following: . Sample a proposal $ theta_*$ using a proposal distribution: $ theta_* sim J( theta_* | theta_{n-1})$ (for example a multivariate Normal distribution: $J( theta_* | theta_{n-1}) = N( theta_* | theta_{n-1}, alpha I)$) | Compute &quot;how much better this proposal is than the previous: | $$ r = frac{P( theta_*|D)}{P( theta_{n-1}|D)} $$ . Set | | $$ theta_n = begin{cases} theta_* &amp; text{with probability} text{ min}(1,r) theta_{n-1} &amp; text{if not} end{cases} $$There is one technical constraint on this proposal distribution $J$ and that is that it should be symmetric. That means it should be just as likely to go in either direction: $J( theta_a | theta_b) = J( theta_b | theta_a)$. Our example above, a multivariate normal with the previous step as mean, satisfy this requirement. . So why should this thing work? First, see that if we get a better set of parameter then $P( theta_*|D) &gt; P( theta_{n-1}|D)$ and the ratio is above 1. Then we will always move to the new value (with probability 1)! That is comforting. If I stand somewhere on a smooth mountain and only take a step whenever that step is upwards I can be pretty certain I reach the top! . But what about that second term? What if we actually dont improve? Looking at the algorithm, that is the case when r is less than 1. We then have a positive possibility of moving anyways. Intuitively this is important for two reasons: . We want to find a posterior distribution at the end. If we only moved whenever we improved we would end up with a final value. Actually moving in slightly worse directions will allow our parameters to wiggle around the optimal solution a little bit. And luckily, this kind of wiggling will give us the posterior distribution! | Having a positive probability of moving in wrong directions also gives us a chance to avoid local minima. In fact, since our proposal distribution has a positive probability of jumping to any set of parameter values from any point, one can prove that the metropolis mcmc algorithm will find the optimum if it just runs for long enough. | Step size aka learning rate . Given that we choose a gaussian proposal distribution with mean of the previous parameter set, we still need to set the covariance matrix of the distribution. In the example above it was $ alpha I$, where $ alpha &gt; 0$ and $I$ is the identity matrix. Given this parameterization $ alpha$ determines how far we should try to jump in the metropolis algorithm. A large $ alpha$ will make us often jump far, whereas a small $ alpha$ will make us jump shortly. If we do short jumps we are likely to get proposal parameters that does not perform too bad and the metropolis algorithm will often accept the new parameters, but we will take very short steps each time. On the other hand, if we do large jumps we may sometiems get very good gains, but they will also be very bad very often. Therefore larger values of $ alpha$ will cause the accept probability to be low, but with large gains whenver it accepts. . This trade off looks very similar to learning rates in deep learning: Small steps will converge but slowly, and large steps takes giant leaps of faith and may not converge very well. The difference is of course that the metropolis algorithm will just not accept any proposals if you set $ alpha$ too high. There are more complicated algorithms that tries to auto-set $ alpha$, but for now we will just find a reasonable value (by trial and error) and stick to it. . Implementation of Metropolis algorithm . Let us implement a small &quot;optimizer&quot; that finds the posterior distribution using the metropolis algorithm. The main component of this class is the step() function, and it goes through the steps above. In addition we have implemented a fit() function for the training loop and a dictionary that holds all the parameters values over iterations so we can look at them later: . class MetropolisOptimizer(): def __init__(self, net, alpha): super().__init__() self.net = net self.alpha = alpha @torch.no_grad() def step(self, data=None): # Step 1: proposal_net = copy.deepcopy(self.net) # we copy the whole network instead of just the parameters for simplicity for name, par in proposal_net.named_parameters(): newpar = par + torch.normal(torch.zeros_like(par), self.alpha) par.copy_(newpar) # Step 2: calculate ratio ratio = torch.exp(proposal_net.logpost(data) - self.net.logpost(data)) # Step 3: update with some probability: if (random.random()&lt;ratio).bool(): self.net = proposal_net else: pass return self.net def fit(self, data=None, num_steps=1000): # We create one tensor per parameter so that we can keep track of the parameter values over time: self.parameter_trace = { key : torch.zeros( (num_steps,) + par.size()) for key, par in self.net.named_parameters()} for s in range(num_steps): current_net = self.step(data) for key, val in current_net.named_parameters(): self.parameter_trace[key][s,] = val.data model = BayesNet() trainer = MetropolisOptimizer(model, alpha=0.02) # Train the model (and take the time): %time trainer.fit(data, num_steps=5000) . CPU times: user 3.73 s, sys: 21 µs, total: 3.73 s Wall time: 3.73 s . We only have 3 parameters and can visualize all of them. We see that all three variables first trods its way towards the area were its &quot;supposed to be&quot; and then starts wiggling around that area. This is the posterior distribution! In addition, we visualize a two dimensional plot of where $ beta_0$ and $ beta$ are over all steps: . All parameters starts in some random position and then quickly iterates itself towards where the posterior distributions should be. In the last plot we can see this over the two parameters $ beta$ and $ beta_0$ where they start in the lower right area and end up around there posteiror distribution in the upper left region. . The iterations before the model has converged to its posterior distribution (happens at around step 1500 or so) is called the burn-in phase, or it would just be called &quot;training&quot; in a standard deep learning setting. However, in bayesian deep learning, this is where the fun starts, because now the mcmc starts to describe the full posterior distribution. These two phases are very clear in the lower right plot! . Langevin diffusion and approximate gradients . Metropolis MCMC is a fairly simple algorithm. It even requires us to do a step in a random direction at each step, when its obvious we can often do better. For instance, it would be smart to use the gradient of our distirbution function to move towards an area where $P( theta| D)$ is larger. . The building block for doing so is the Langevin diffusion. Let us first write the unnormalized posterior as $P( theta | D) sim e^{-U( theta)}$, where we have introduced the potential energy function $U( theta) := -log P(D| theta) - log P( theta)$. Usually this function can be decomposed into a sum over the prior and all data points: $U( theta) = sum_{n=1}^N U_i( theta) := sum_{n=1}^N -log(f(y_i| theta)) - frac{1}{N}log(P( theta))$. In our motivating example U simply becomes $U( theta) = - sum_{n=1}^N log(N(y_n | beta_0 + beta x_n , sigma^2))$. . We can define the langevin diffusion, which is a stochastic differential equation: begin{equation} theta(t) = - frac{1}{2} nabla U( theta(t))dt + dB_t end{equation} . Stochastic differential equations are intutively the same as normal differential equation, just that they have a randomness element $dB_t$ (a brownian motion). We will not dwelve too much about that here, but think of it simply as a random walk that disturbs the direction we set in the first term. . So why do we talk about this langevin differential equation? It turns out that the stationary distribution of the equation above is in fact the posterior distribution we are looking for! So, if we are able to simulate that process over a long time we will actually get samples from the posterior distribution! Of course, solving differential equations are difficult to do analytically. However, just like normal differential equations we can approximate a stochastic differential equation by an linear approximation using a small step size $ alpha$: . begin{equation} theta(t+ alpha) = theta(t) - frac{ alpha}{2} nabla U( theta(t)) + alpha*z end{equation}where $z$ is a d-dimensional standard gaussian random variable. . By simulating from this approximation we can obtain the posterior distribution. This equation looks surprisingly similar to the gradient descent algorithm: We update the current parameters by moving a small step towards the gradient of our objective. There are two main differences: The langevin diffusion adds a small noise factor $ alpha z$. It also requires us to evaluate the gradient of $U$ for all datapoints. This is unpractical on many datasets and the deep learning litterature usually approximate the gradient by an unbiased estimate at each iteration by subsampling the data $$ hat{ nabla} U( theta) = frac{N}{|S|} sum_{i in S} nabla U_i( theta) $$ where $S$ is a random sample of $|S|$ number of datapoints. . That gives us the SGLD algorithm! We simply simulate the langevin diffusion using small step sizes $ alpha$ and an estimate of the gradient $ hat{ nabla} U( theta)$ by subsampling datapoints. It is important to note that these two approximations (the discretization and the gradient estimation of the data) will not give us the true posterior distribution. If we wanted to do this correctly, we should have added an accept/reject step in the algorithm. However, this will often be costly, as we would have had to estimate the full $U( theta)$ whenever we wanted to do that. . Implementation of the SGLD algorithm . The implementation of SGD algorithm requires us the calculate the gradient of the likelihood function. That is something deep learning frameworks are very good at. On the other hand, the SGLD algorithm will always accept the proposed parameters, making the optimization algorithm much simpler than the previous metropolis algorithm (at the risk of being a bad approximation to the posterior). For this implementation, note that we do not subsample the data, as we only have 30 anyways. . from torch.optim.optimizer import Optimizer class TrainOptimizerSGLD(Optimizer): def __init__(self, net, alpha=1e-4): super(TrainOptimizerSGLD, self).__init__(net.parameters(), {}) self.net = net self.alpha = alpha def step(self, batch, batch_size=1, num_data=1): self.zero_grad() weight = num_data/batch_size loss = -self.net.loglik(batch)*weight - self.net.logprior() loss.backward() with torch.no_grad(): for name, par in self.net.named_parameters(): newpar = ( par - 0.5*self.alpha*par.grad + torch.normal(torch.zeros_like(par), std=self.alpha) ) par.copy_(newpar) return loss def fit(self, data=None, num_steps=1000): # We create one tensor per parameter so that we can keep track of the parameter values over time: self.parameter_trace = {key : torch.zeros( (num_steps,) + par.size()) for key, par in self.net.named_parameters()} for s in range(num_steps): loss = self.step(data) for key, val in self.net.named_parameters(): self.parameter_trace[key][s,] = val.data model = BayesNet() trainer = TrainOptimizerSGLD(model, alpha=0.02) # Train the model (and take the time): %time trainer.fit(data, num_steps=5000) plot_parameter_trace(trainer) . CPU times: user 3.8 s, sys: 18.6 ms, total: 3.82 s Wall time: 3.82 s . Compared to the metropolis algorithm we see that the beta parameters quickly snap into place. However, the sigma parameter is varying much more than anticipated. This must be due to the approximation error in the discretization of the stochastic differential equation. Reducing step size should solve this problem: . model = BayesNet() trainer = TrainOptimizerSGLD(model, alpha=0.005) # Train the model (and take the time): %time trainer.fit(data, num_steps=5000) plot_parameter_trace(trainer) . CPU times: user 3.84 s, sys: 1.75 ms, total: 3.84 s Wall time: 3.84 s . Voila! Compared to the Metropolis algorithm, the SGLD snapped the $beta$ parameters pretty much straight into the correct area. It shows a great speed improvement from the random walk algorithm of Metropolis, something which is cruical for more complicated problems. At the same time, we can see that $ sigma$ never really converged to its true value around 0.5, but keeps jumping up. This must be due to the Euler approximation in the Langevin diffusion, and should be solveable by reducing step sizes. Let us repeat the SGLD algorithm with a smaller step size. We see that although the convergence is slightly slower (but much faster than the metropolis!), we end up with a better approximation: . Cyclical SG-MCMC (cSG-MCMC) . The last algorithm we shall go through is a recent ICML paper that utilizes cyclical stepsizes to capture a &quot;better&quot; posterior distribution. They focus on the problem when a posterior distribution have multiple modes, or optimal &quot;areas&quot;, in the parameter space. This problem is typical for neural nets with their large parameter space. . The Cyclical SG-MCMC algorithm works as the SGLD algorithm above, but using varying step sizes. The algorithm starts with a large step size (exploration mode) to quickly move towards an interesting mode then decreasing the step size gradually to capture the posterior distribution around the found mode (sampling mode). It repeats this procedure multiple times, being able to capture a multi-modal posterior distribution. Finally it uses all parameter samples found in sampling mode as its posterior distributions. . . The technique is similar to other MCMC algorithms that tries to reinitialize the MCMC chain in different points to find different modes. However, this paper argues that using cycles will warm-start the network at each iteration, allowing it to start in a slightly smarter place and saving computational cost for the same performance. . Empirically they show that cSG-MCMC are able to find multiple modes whereas SGLD with 4 parallel iterations can only find 4: . And they show that cSG-MCMC can achieve lower test errors than benchmark optimization algorithms: . Scaling sg-mcmc up to neural networks . So far we&#39;ve been analyzing a simplistic example, but both cSG-MCMC and SGLD are clearly meant for larger datasets and neural networks. We therefore change our dataset to the CIFAR-10 dataset, an image dataset with 10 classes. Our likelihood is the Categorical Distribution, and we get the correct energy function $U( theta)$ by applying the nn.CrossEntropy loss. We utilize pytorch-lightning to structure the model, but the main ingredients are in the optimizer. . dataset = CIFAR10DataModule(data_dir=&quot;.&quot;) # visualize a batch x,y = next(iter(dataset.train_dataloader())) _ = plt.imshow(torchvision.utils.make_grid(x).permute(1,2,0)) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-12T20:05:35.733871 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ class OptimizerSGLD(Optimizer): &quot;&quot;&quot; Optimizer that allows for cyclic learning rate, stochastic mcmc and normal sgd.&quot;&quot;&quot; def __init__(self, net, lr_max=1e-3, lr_min=1e-5, max_step = 10e3, noise=True, noise_part2=True): super(OptimizerSGLD, self).__init__(net.parameters(), {}) self.net = net self.lr_max = lr_max self.lr_min = lr_min self.lr = self.lr_max self.max_step = max_step self.it=0 self.noise = noise self.noise_part2 = noise_part2 # parameter overwriting noise to only have it for sampling stage @torch.no_grad() def step(self, batch_size=1, num_data=1): # Update learning rate: self.it +=1 ratio = (self.it % self.max_step)/self.max_step self.lr = self.lr_max*(1-ratio) + self.lr_min*ratio self.net.log(&quot;learning_rate&quot;, self.lr) if self.noise_part2: if ratio &gt; 0.5: self.noise = True else: self.noise = False # Update parameters: for name, par in self.net.named_parameters(): newpar = par - 0.5*self.lr*par.grad if self.noise: newpar += torch.normal(torch.zeros_like(par), std=self.lr) par.copy_(newpar) . class LitMNIST(LightningModule): def __init__(self, lr_max=1e-3, lr_min=1e-5, max_step = 10e3, noise=True, noise_part2=True): super().__init__() self.layer_1 = nn.Conv2d(3, 16, 3) self.layer_2 = nn.Conv2d(16, 32, 3) self.linear1 = nn.Linear(1152, 128) self.linear2 = nn.Linear(128, 10) self.criterion = nn.CrossEntropyLoss() # Metrics self.train_acc = pl.metrics.Accuracy() self.val_acc = pl.metrics.Accuracy() # Initialize optimizer self.opt = OptimizerSGLD(self, lr_max, lr_min, max_step, noise=noise, noise_part2=noise_part2) def forward(self, x): batch_size, channels, width, height = x.size() x = F.relu(self.layer_1(x)) x = F.max_pool2d(x,2) x = F.relu(self.layer_2(x)) x = F.max_pool2d(x,2) x = x.view(batch_size,-1) x = F.relu(self.linear1(x)) x = self.linear2(x) return x def step(self, batch, batch_idx, phase): x, y = batch logits = self(x) loss = self.criterion(logits, y) self.log(f&quot;{phase}/loss&quot;, loss) if phase == &quot;train&quot;: self.train_acc(logits,y) if phase == &quot;val&quot;: self.val_acc(logits,y) return loss def training_step(self, batch, batch_idx): return self.step(batch, batch_idx, &quot;train&quot;) def validation_step(self, batch, batch_idx): for key, val in self.named_parameters(): self.log(f&#39;param/{key}&#39;, value = val.abs().mean()) return self.step(batch, batch_idx, &quot;val&quot;) def training_epoch_end(self, outs): self.log(&#39;train/accuracy&#39;, self.train_acc.compute()) self.log(&quot;learning_rate&quot;, self.optimizers().lr) def validation_epoch_end(self, outs): self.log(&#39;val/accuracy&#39;, self.val_acc.compute()) def configure_optimizers(self): return self.opt . Comparing cSGMCMC vs SGLD vs SGD . Given our optimizer above it is easy to modify it to run different algorithms, and we will compare these three algorithms in a simple run. . configs = { &#39;cSGMCMC-temp&#39; : { &#39;max_step&#39; : 30e3, &#39;noise&#39; : True, &#39;noise_part2&#39; : True }, &#39;cSGMCMC&#39; : { &#39;max_step&#39; : 30e3, &#39;noise&#39; : True }, &#39;SGLD&#39; : { &#39;max_step&#39; : 1564*100, &#39;noise&#39; : True }, &#39;SGD&#39; : { &#39;max_step&#39; : 1564*100, &#39;noise&#39; : False }, } for name, config in configs.items(): model = LitMNIST(lr_max=1e-3, lr_min=1e-5, **config) logger = TensorBoardLogger(&quot;logs&quot;, name=name) trainer = Trainer(gpus=1, progress_bar_refresh_rate=0, logger=logger, max_epochs=100) trainer.fit(model, dataset) print(f&quot;Finished training {name}&quot;) . GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Files already downloaded and verified Files already downloaded and verified | Name | Type | Params -- 0 | layer_1 | Conv2d | 448 1 | layer_2 | Conv2d | 4 K 2 | linear1 | Linear | 147 K 3 | linear2 | Linear | 1 K 4 | criterion | CrossEntropyLoss | 0 5 | train_acc | Accuracy | 0 6 | val_acc | Accuracy | 0 GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -- 0 | layer_1 | Conv2d | 448 1 | layer_2 | Conv2d | 4 K 2 | linear1 | Linear | 147 K 3 | linear2 | Linear | 1 K 4 | criterion | CrossEntropyLoss | 0 5 | train_acc | Accuracy | 0 6 | val_acc | Accuracy | 0 Finished training cSGMCMC GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params -- 0 | layer_1 | Conv2d | 448 1 | layer_2 | Conv2d | 4 K 2 | linear1 | Linear | 147 K 3 | linear2 | Linear | 1 K 4 | criterion | CrossEntropyLoss | 0 5 | train_acc | Accuracy | 0 6 | val_acc | Accuracy | 0 Finished training SGLD Finished training SGD . Results . . . Illustration of how to implement the algorithms, unfortunately not sufficient time to evaluate the different optimization algorithms properly. Some findings: . We would in this case expect that SGD performs better than the alternatives as SGD is finding the maximum aposterior $ theta_{MAX} = argmax_{ theta} P(D| theta)$, and the others are find a posterior distribution &quot;around&quot; that. Seems confirmed by comparing SGD and SGLD. | . | We see that cSGLD is oscilating, each of the optimas should, according to the cSGLD paper be different modes of the distribution. If we were to estimate the loss by ensembling the whole distribution, the test loss should be lower than for SGD and SGLD. We did not have time for this. | . | .",
            "url": "https://simeneide.github.io/blog/bayesian%20neural%20net/2020/11/06/bayesian-deep-learning.html",
            "relUrl": "/bayesian%20neural%20net/2020/11/06/bayesian-deep-learning.html",
            "date": " • Nov 6, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Running Google Colab with VS code",
            "content": "Running VScode and the python extension is great for development. I get clean python files and can run my code interactively. It is the same setup we have at work and I then remotely connect to a server with more cpu and GPU. However, for my side gigs I havent really figured out a workflow, until now. . This morning I found colab-ssh. It enables you to remotely connect your google colab instance to your local VScode! And if you have a GPU runtime on google colab you get that as well, of course. Combining this with a small google drive mounting, and I get more or less my local working environment but with GPU acceleration. . . A quick step-by-step (see colab-ssh for updates if this doesnt work): . Open a new google colab notebook. | In first cell: Mount your google drive using these commands (you need to follow link and authorize): from google.colab import drive drive.mount(&#39;/root/gdrive&#39;) . | Go to this site and get an ngrok token. | Second cell: Add your token and create a password. Then add+run this: | # Install colab_ssh on google colab !pip install colab_ssh --upgrade ngrokToken = &#39;XXX&#39; password = &#39;XXX&#39; from colab_ssh import launch_ssh, init_git launch_ssh(ngrokToken,password) . You’ll now see something like this: | Collecting colab_ssh Downloading https://files.pythonhosted.org/packages/a7/c5/eedfd8b374fead9d863cb7031d9dc97fed50003372922ba0efd85d9fe3e0/colab_ssh-0.2.63-py3-none-any.whl Installing collected packages: colab-ssh Successfully installed colab-ssh-0.2.63 Successfully running 2.tcp.ngrok.io:13254 [Optional] You can also connect with VSCode SSH Remote extension using this configuration: Host google_colab_ssh HostName 2.tcp.ngrok.io User root Port 12345 . Go to your local VScode and select Remote-SSH: Open Configuration File, and paste the config above: | . . Select Remote-SSH: Connect to Host and select the google colab ssh connection. | . Voila! Up and running with gpu and your google drive attached. . Unfortunately, the hostname and port changes each time, and you still have to manually open the google colab. . Still, really great work by Wassim Benzarti. .",
            "url": "https://simeneide.github.io/blog/vscode/2020/09/14/colab-vscode-gpu.html",
            "relUrl": "/vscode/2020/09/14/colab-vscode-gpu.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Bayesian Inference Techniques",
            "content": "+++ date = 2020-08-05 title = “Bayesian Inference techniques for deep neural nets” active = true tags = [“foo”] +++ . “Why posterior distribution? I just want my neural net to be good” . General setup . We have a dataset $D = [{x_i, y_i }, i=1,…,n]$, and have defined a likelihood function $P(D| theta)$ where $ theta in R^d$ are some parameters defined in the model. We have also already defined a prior for these parameters $P( theta)$. The posterior distribution can then be written as . P(θ∣D)=P(D∣θ)∗P(θ)P(D)∝P(D∣θ)∗P(θ)P( theta | D) = frac{P(D| theta) * P( theta)}{P(D)} propto P(D| theta) * P( theta)P(θ∣D)=P(D)P(D∣θ)∗P(θ)​∝P(D∣θ)∗P(θ) . Finding $P(D)$ is impossible for all but the simplest cases, and as we are interested in complicated likelihood functions such as neural networks, it will clearly not work for us. Therefore, we only have the posterior distribution up to a constant. Luckily, there has been developed multiple techniques that can find an approximation to the posterior distribution that only requires There exist multiple techniques to infer the posterior distribution of a bayesian neural network: . Variational Inference | Dropout | SWAG | Markov Chain Monte Carlo | Stochastic Markov Chain Monte Carlo (SG-MCMC) | . Markov Chain Monte Carlo (MCMC) . MCMC is the “traditional” way of finding the posterior distribution. It is an iterative algorithm (the markov chain name) that is updating the belief of what the model parameters $ theta$ should be. I.e. we end up with a loong chain of parameter values: $ theta_1, theta_2, …, theta_{n}$. However, we are not looking for a fixed value, but a distribution. Luckily, there are theory that tells us that if you run this sequence for long enough, then all values $ theta_n$ will be samples from the posterior distribution. Voila! . So how does it work? . Metropolis MCMC algorithm . The metropolis MCMC algorithms does nothing but starting in some random position $ theta_0$ and randomly tries to go in a direction (e.g. add a Normal distribution $ theta_1 = theta_0 + N(0,1)$). If we improved (meaning that $P( theta | D)$ increased) we use that new value. If it didnt improve, we may still move to that point depending on some probability. . [ADD A DRAWING HERE THAT SHOWS ] . Formally the algorithm looks like this: . Initialize a starting point $ theta_0$. This can either be random or somewhere you think is reasonable. For instance, you can start in the center of your prior distribution $argmax_{ theta} P( theta)$. | For each $n=1,2,3,…$ do the following: Sample a proposal $ theta_$ using a proposal distribution: $ theta_ sim J( theta_* | theta_{n-1})$ (for example a multivariate Normal distribution: $J( theta_* | theta_{n-1}) = N( theta_* | theta_{n-1}, I)$ | . | Compute “how much better this proposal is than the previous: | r=P(θ∗∣D)P(θn−1∣D)r = frac{P( theta_*|D)}{P( theta_{n-1}|D)}r=P(θn−1​∣D)P(θ∗​∣D)​ Set | θn={θ∗with probability min(1,r)θn−1if not theta_n = begin{cases} theta_* &amp; text{with probability} text{ min}(1,r) theta_{n-1} &amp; text{if not} end{cases}θn​={θ∗​θn−1​​with probability min(1,r)if not​ | There is one technical constraint on this proposal distribution $J$ and that is that it should be symmetric. That means it should be just as likely to go in either direction: $J( theta_a | theta_b) = J( theta_b | theta_a)$. Our example above, a multivariate normal with the previous step as mean, satisfy this requirement. . So why should this thing work? First, see that if we get a better set of parameter then $P( theta_*|D) &gt; P( theta_{n-1}|D)$ and the ratio is above 1. Then we will always move to the new value (with probability 1)! That is comforting. If I stand somewhere on a smooth mountain and only take a step whenever that step is upwards I can be pretty certain I reach the top! . But what about that second term? What if we actually dont improve? Looking at the algorithm, that is the case when r is less than 1. . Learning rate . Variational Inference . Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) .",
            "url": "https://simeneide.github.io/blog/2020/08/05/bayesian-inference-techniques.html",
            "relUrl": "/2020/08/05/bayesian-inference-techniques.html",
            "date": " • Aug 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Installing Pytorch on a raspberry pi 4",
            "content": "Update 15 sept 2020 . I found these wheel builds from Thomas Viehmann that worked very well on a rpi4 64 bit running python 3.7. They are pytorch 1.6.0 and avoids the original hacks. Only issue was that my camera stopped working, but manage to circumvent it by using a different driver (v4l-utils) and using opencv’s VideoCapture() to get images. Got it running in a docker image with the following (removed some parts that I dont think is necessary): . (the balena docker image is a fairly stripped down ubuntu image) . FROM balenalib/raspberrypi4-64:latest # Defines our working directory in container WORKDIR /usr/src/app RUN sudo apt-get update RUN apt-get install -y gcc python3-dev v4l-utils python3-opencv python3-pip python3-setuptools libffi-dev libssl-dev # PYTORCH: RUN wget https://mathinf.com/pytorch/arm64/torch-1.6.0a0+b31f58d-cp37-cp37m-linux_aarch64.whl RUN wget https://mathinf.com/pytorch/arm64/torchvision-0.7.0a0+78ed10c-cp37-cp37m-linux_aarch64.whl RUN sudo apt-get install -y python3-numpy python3-wheel python3-setuptools python3-future python3-yaml python3-six python3-requests python3-pip python3-pillow RUN pip3 install torch*.whl torchvision*.whl . Original Post . Earlier this year I had to install pytorch on a raspiberry pi for my robotic lawn mower project (more on that later). However, the process was very painful, so Ill throw my notes here in case anyone else tries to do the same. Its not supposed to be bullet-proof, but may help with some pointers. Updates to this proceudre may be found here. . Installed from wheel on these: https://github.com/nmilosev/pytorch-arm-builds . But for rpi4 there was some errors, so I installed a wheel after reading this comment: https://github.com/nmilosev/pytorch-arm-builds/issues/4#issuecomment-527433112 . Install from his wheel a bit longer down the thread, and rename those _C..so and _d..so files to _C.so and _d.so. . Torchvision works, but Pillow 7.0.0 was too new, so downgraded to 6.1 after some random comments I found. . Step-by-step: . PIP install pytorch from wheel . Download wheel from here https://github.com/nmilosev/pytorch-arm-builds and run sudo pip3 install torch-1.1.0-cp37-cp37m-linux_armv7l.whl . Rename some files . Then if you try to run sudo python3 -c &quot;import torch&quot; you get: . Traceback (most recent call last): File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;/usr/local/lib/python3.7/dist-packages/torch/__init__.py&quot;, line 79, in &lt;module&gt; from torch._C import * ModuleNotFoundError: No module named &#39;torch._C&#39; . Can be fixed by the following: . cd /usr/local/lib/python3.7/dist-packages/torch sudo mv _C.cpython-37m-arm-linux-gnueabi.so _C.so sudo mv _dl.cpython-37m-arm-linux-gnueabi.so _dl.so .",
            "url": "https://simeneide.github.io/blog/rpi4/pytorch/2020/06/30/pytorch-raspberry.html",
            "relUrl": "/rpi4/pytorch/2020/06/30/pytorch-raspberry.html",
            "date": " • Jun 30, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "PILCO and Deep PILCO",
            "content": "Papers: . Deisenroth, M. P., &amp; Rasmussen, C. E. (2011). PILCO: A model-based and data-efficient approach to policy search. Proceedings of the 28th International Conference on Machine Learning, ICML 2011, 465–472. | Gal, Y., Mcallister, R. T., &amp; Rasmussen, C. E. (2016). Improving PILCO with Bayesian Neural Network Dynamics Models. Data-Efficient Machine Learning Workshop, ICML, 1–7. | . The papers shows how to find good policies with relatively few observations on classical control problems (mountain car, pole swing up etc) using probabilistic model based reinforcement learning. . Being model based in reinforcement learning means that you build a statistical model of the environment, a world model. The RL algorithm can then search for an optimal policy by simulating from this world model. This is more data efficient than in the “model free” reinforcement learning algorithms, where one needs millions of examples to learn relatively simple games. However, model based simulators suffers from their approximations to the real world, which often ends up as huge errors when you simulate multiple steps. By introducing a probabilistic dynamics model, the PILCO algorithms tries to account for the fact that future trajectories are uncertain by introducing parameter uncertainty. The original PILCO paper does this by using Gaussian Processes, while the deep PILCO use a neural net with the “dropout trick” to approximate a bayesian neural net. . . The framework assumes that the world model is of the form . $x_t=f(x_{t-1},u_{t-1})$ . where $x in R^d$ is a continuous state of the world at time t, $u_t in R^F$ is an action at time t, and f is some function of the real world transition dynamics. . The objective is to find a policy $ pi(x) = u$ that minimize the expectation of a cost function $c(x_t)$ over all time steps: . . The policy is found by iterating between learning the posterior of the world transition dynamics and simulating using the dynamics and optimizing the policy: . . The original paper uses a gaussian process to model $f(x_t)$, which gives an analytical solution for the posterior. However, gaussian processes does not scale well for large datasets, and the Deep PILCO paper instead uses a neural net to estimate the same dynamics. They approximate the posterior using variational inference and minimize the KL-divergence through using dropout, which can be interpreted as a variational bayesian approximation. . Both papers show that their algorithm is more data efficient than current state of the art reinforcement learning algorithms: . .",
            "url": "https://simeneide.github.io/blog/rl/2019/01/17/pilco+improve.html",
            "relUrl": "/rl/2019/01/17/pilco+improve.html",
            "date": " • Jan 17, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Bpmf Paper",
            "content": "+++ date = 2018-10-06 title = “BPMF presentation” active = true tags = [“foo”] +++ . Slides from my presentation on “Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo” by Salakhutdinov and Mnih. Paper link. .",
            "url": "https://simeneide.github.io/blog/2018/11/23/bpmf-paper.html",
            "relUrl": "/2018/11/23/bpmf-paper.html",
            "date": " • Nov 23, 2018"
        }
        
    
  
    
        ,"post6": {
            "title": "Recsys2018",
            "content": "+++ date = 2018-10-06 title = “Takeaways Recsys 2018” active = true tags = [“foo”] +++ . For the first time Ive actually made a summary of all the papers and presentations I found noteworthy at a conference (allright, there were more, but this is a start). Below is my notes, with links etc. The purpose of the notes is mainly for myself to remember and revisit what I found interesting, but I see no reasons not to share to others. Does not include my own paper. . Keynote dlrs - Joachims: Deep learning with logged bandit feedback . Paper: http://www.cs.cornell.edu/people/tj/publications/joachims_etal_18a.pdf | idea: utilize current policy to build a better contextual bandits to recommend. | using Inverse Propensity Scoring | Using Self-normalizing IPS estimator (SNIPS) | Also using self normalizing | . . . . A very similar talk seem to be posted here: . Youtube, Minmin Chen: Off-policy correction for a REINFORCE Rec system . Invited talk on the evaluation workshop REVEAL | Youtube shared their work on how the models they used now works, focus on evaluation | Maximized recommendations based on expected future reward | they do not use td to train the model, rather the all the actual future rewards and discounts them to get training signal. | . . . Corrects off-policy updates with importance sampling/IPS . | Exploration is costly, so they tune ut with an extra weight that they set. | Recommends a set of top K items. Assume additive rewards: | . . Results . Improved recs, also able to recommend more items into the tail. | . . Recsys-competition winners: Two-stage model for Automatic Playlist Continuation at Scale . Challenge website: http://www.recsyschallenge.com/2018/ | Winner paper: http://www.cs.toronto.edu/~mvolkovs/recsys2018_challenge.pdf | Task was to complete a user playlist at spotify. | A playlist may have be created over a long time. | Given the first items, predict the last ones. | . Approach: . step 1: Reduce candidate set of all items to 20k by using a temporal convolutional layer. (lstm worked too, but was slower to train and iterate on). | The step 1 was really about maximizing recall. | . | Step 2: xgboost classifier on these candidates | . . Categorical-attributes-based item classification for recommender systems: hiarchical softmax and multi loss . Paper: https://dl.acm.org/citation.cfm?id=3240367 | Setting: Next item prediction with items within some category structure. | Using negative sampling during training . | Does the recommender improve by predicting a hiarchical softmax instead of doing multi target prediction? . | Result: Using hiarchical modeling is better than multi target. Testet with MAP@5 on recsys16 dataset and “large propertary dataset”. | Also helps with cold start | . Own comments: Unsure of the improvement is due to negative sampling or that you infer more structure in your data/model. . &quot;Categorical-Attributes-Based Item Classification for Recommender Systems&quot; by @QianZhao3 and Google folks lead by @edchi - really interesting idea: categorical labels as *outputs* of multitask model you are optimizing when recommending items #RecSys2018 https://t.co/Bkljw0zVFf pic.twitter.com/PavBxdtLaX . &mdash; Xavier @ #recsys2018🎗🤖🏃 (@xamat) October 5, 2018 . . Keynote 2 dlrs - Ray Jiang, deepmind: slate recommendation (part 1) . Relevant paper (seems unpublished): https://arxiv.org/pdf/1803.01682.pdf | predict a full feed instead of single items | use a VAE to do this, | “Works really well”. | Tested on Recsys 2015: was the best slate dataset they could find | . . . https://arxiv.org/abs/1803.01682 . Calibrated Recommendations . Paper: https://dl.acm.org/citation.cfm?id=3240372 | If you have seen 70% drama and 30% horror, optimizing a recommender on precision, the best solution is to give you 100% drama and get 70% precision. | The paper suggests to calibrate the recommendations to be more representative. | Done by regularizing the recommendations with the KL divergence of categories (genres in this case) | Done as a post processing step. | Result: Can rerank top recommendations to a much more representative distribution without losing accuracy. | . https://dl.acm.org/citation.cfm?id=3240372 . Explore, Exploit, and Explain: Personalizing Explainable Recommendations with Bandits . Paper: https://dl.acm.org/citation.cfm?id=3240354 | Feed-bandit that uses a factorization machine to predict and explain recommendations | context: Home page of spotify account. Different shelves of recommendations, each with an explanation (“because you recently listened to..”) | . Invited talk Netflix: Correlation &amp; Causation . Yves Raimond, AI director Netflix | Netflix’s approach: personalize everything | Made some thoughts about the do operator P(Y | do(X)) | . | Maybe better for recs to focus on P(click | do(X)) - P(click | not do X) | . | IPS: | pro: simple, model-agnostic | con: only unbiased if no unobserved cofounders, high variance | Alternative til IPS: Instrumental Variable | used in econometrics | pro: robust to unobserved cofounders | con: bias/var depends on strength of IV, hard to scale | . . . Generation meets recommendation - “Generating new items that fit most users” . Paper: https://arxiv.org/abs/1808.01199 | . “Consider a movie studio aiming to produce a set of new movies for summer release: What types of movies it should produce? Who would the movies appeal to?” | “Specifically, we leverage the latent space obtained by training a deep generative model—the Variational Autoencoder (VAE)—via a loss function that incorporates both rating performance and item reconstruction terms.” | “We then apply a greedy search algorithm that utilizes this learned latent space to jointly obtain K plausible new items, and user groups that would find the items appealing.” | . On the robustness and discriminative power of information retrieval metrics for top-N recommendation . Paper: https://dl.acm.org/authorize.cfm?key=N668684 | An evaluation of robustness of many offline metrics at different ranking level. E.g. MRR@5, Recall@10, MAP@100, … . | Takeaway 1: Use a high cutoff (e.g. 100 instead of 10) when doing offline evaluations, like MRR. The metric is more robust, and highly correlated to the MRR@10 values | Takeaway 2: MRR is one of the lesser robust offline metrics. | . . Takeaway 2: mrr er ganske lite robust. . Unbiased offline recommender evaluation for missing-not-at-random implicit feedback . Implicit-feedback Recommenders (ImplicitRec) leverage positive only user-item interactions, such as clicks, to learn personalized user preferences. Recommenders are often evaluated and compared offline using datasets collected from online platforms. These platforms are subject to popularity bias (i.e., popular items are more likely to be presented and interacted with), and therefore logged ground truth data are Missing-Not-At-Random (MNAR). . “Average over all” estimators are biased in Implicit rec datasets | Use IPS to evaluate policies. | reduce bias with 30% in a yahoo! music datset. | Paper: https://dl.acm.org/citation.cfm?id=3240355 | . RecoGym . Simulation environment where you can evaluate your recommender agent | Follows the same style as openAI gym: env.step(action) | When we tried it a bit the day before, the users seemed to click on the same items over and over again, probably some tuning that needs to be done there? | This is sort of an alternative approach to offline evaluation. Simulators are limited by their generating model, but can we still use it to test algorithms for convergence etc? . | Unrelated to talk and recogym, but some notes me and Olav did on rec simulations during conf. Same ideas: | . . News session-based recommendations using DNN . A recommendation algorithm to recommend news. | Freshness and coldstart big problems. | Separate item representation that uses a lot of content, independent of users | Unfortunately not tested in prod (authors from large news corp.) | Tested on offline data: Beats everything, incl gru4rec ++ | Paper: https://arxiv.org/abs/1808.00720 | Code: https://github.com/criteo-research/reco-gym | . . What happens if users only share last n days of data? (Exploring recommendations under user-controlled data filtering) . “Using the MovieLens dataset as a testbed, we evaluated three widely used collaborative filtering algorithms.” | “Our experiments demonstrate that filtering out historical user data does not significantly affect the overall recommendation performance.” | Impacts those who opted out (naturally) | . Paper: https://scholar.google.com/citations?user=Vyj2jeoAAAAJ&amp;hl=en#d=gs_md_cita-d&amp;p=&amp;u=%2Fcitations%3Fview_op%3Dview_citation%26hl%3Den%26user%3DVyj2jeoAAAAJ%26citation_for_view%3DVyj2jeoAAAAJ%3A2osOgNQ5qMEC%26tzom%3D420 . Interactive recommendation via deep neural memory augmented contextual bandits . created a recsys simulator? check out… | Paper: https://dl.acm.org/citation.cfm?id=3240344 | .",
            "url": "https://simeneide.github.io/blog/2018/10/06/recsys2018.html",
            "relUrl": "/2018/10/06/recsys2018.html",
            "date": " • Oct 6, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Kdd Workshop",
            "content": "+++ date = 2018-08-19 title = “KDD workshop Deep learning Day: Five lessons from building a deep neural network recommender for marketplaces” active = true tags = [“foo”] +++ . Ning Zhou, Audun Øygard and I got a paper in the KDD workshop Deep Learning Day. We provide some practitioner’s findings on applying deep learning recommendations in production! Link to paper here. . Together with @nzhou9 and @matsiyatzy, I am officially moving into academia after being an industrial observer: We got a paper in the #KDD2018 workshop Deep Learning Day. We provide some practitioner&#39;s findings on applying deep learning recommendations in production! . &mdash; Simen Eide (@simeneide) August 18, 2018 That was fun. pic.twitter.com/KBLjYlvzS9 . &mdash; Simen Eide (@simeneide) August 20, 2018 Poster: .",
            "url": "https://simeneide.github.io/blog/2018/08/19/kdd-workshop.html",
            "relUrl": "/2018/08/19/kdd-workshop.html",
            "date": " • Aug 19, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "Talks And Projects 2017",
            "content": "+++ date = 2018-01-03 title = “Deep Recommenders, Car Pricing, Self driving rc-car and other projects in 2017” active = true tags = [“foo”] +++ . I am not a fan of new years resolutions. If I had been, one of my new years resolutions would be to be better at writing down what I am doing all the time. However, I held some talks and did some fun experiments in 2017, so the cheap way is simply to link to those. . How to become a Data Scientist in 20 minutes (JavaZone 2017) . At Javazone 2017 I held a short talk on how building (drum roll) machine learning algorithms is actually pretty easy. Simply put: I build a regression model that would predict the fair price of a car, and then I explained how I used that model to actually buy the car. The main message was that as long as you do your model validation properly (dont train and test on same data), you dont really need to understand the algorithm to get good results (use random forest!). . We published both algorithm and the dataset here so that anyone could replicate it, and JavaZone even filmed it: . Algorithms we are using in FINN.no to recommend you good stuff . . I have actually held three talks on this. First off was the one I held at Oslo Data Science meetup in February, where I talked about the journey we did from pure CF-models to using Tensorflow. . After that we have tried out a lot of things, and also moved to use keras instead, so we could skip all the boilerplate. Ive held two talks this fall on it: one beer-talk hosted by Bekk Consulting, and one early-morning talk with Bearpoint. The presentations were very similar, and can be downloaded here. . Testing RNN #recommenders for @FINN_tech. The RNN (bottom) generalize better when looking at last 10 items compared to only the last (mid). pic.twitter.com/feZMcz1n97 . &mdash; Simen Eide (@simeneide) August 31, 2017 Workshop for FINN developers on Machine Learning . In November I held a 3 hour workshop with some colleages on machine learning. The idea was, just as in the javazone talk, to demystify building these models. We spun up GPU machines to all the participants, prepared a dataset many FINN ads and a baseline script on how they could classify the ads based on their title. It was around 500’000 ads spread over 20 or so categories. The task was then to understand the baseline algorithm, then improve it by changing architechture, learning rates, optimizers and so on. The baseline started by taking averages of the word2vec vectors of the title, but the winners used a GRU-layer on the title to snitch the last percentage points on the validation accuracy. That was really impressive, none of them had done machine learning before, and then they start tinkering with recurrent neural nets! . Data scientist beware! Sixty developers at @FINN_tech with no prior #MachineLearning experience just build a deep neural classifier in 2.5hrs that beat my model! pic.twitter.com/HmG29pi7mM . &mdash; Simen Eide (@simeneide) November 21, 2017 Clothing GANs . We had a little GAN-workshop at the end of the year here, where we among other things trained a model to generate new clothing. I was impressed how easy that was. Maybe FINN should start generating images of your ad if you cant be bothered to take your own photos? ;) . Do you need to sell off some clothes at @FINN_tech, but not happy with your own image? This is our first try at building a #GAN model that can generate arbitrary clothing images for you! Next steps: upscaling, conditioning and maybe a fashion show? pic.twitter.com/Vkd7takdHT . &mdash; Simen Eide (@simeneide) December 17, 2017 Neural Search Engines . Ive tried to work on models that take text input and outputs a finn ad (aka search engine). They were also enhanced with user-features, so that the search would be personalized to the exact user. It worked all right, but we are currently working on a “simpler” way to personalize FINNs search results, combining the good old search engine with our recommendation models. . Search engines are maybe not the most sexy, but it was really fun to learn a model to predict our #recommendation vectors based on a word! pic.twitter.com/yPDsLkKYxc . &mdash; Simen Eide (@simeneide) September 5, 2017 Self driving rc-car ++ . Ive been tinkering a bit with a self driving car. The project is called donkeycar. Basically it is a rc-car that you can run through a python API with a raspberry pi. They have also integrated tensorflow, so that you can use imitation learning to drive a path. I got it to follow the road, and also a white line. Hopes was to spend enough time to build some reinforcement learning into it, but I haven’t had time (yet!). . Bil + nevralt nett = SELVKJØRENDE BIL! Kan jeg claime Norges første selvkjørende amatørbil?! #autonomousdriving #donkeycar #in #DeepLearning pic.twitter.com/vwZJcfLqLd . &mdash; Simen Eide (@simeneide) August 18, 2017 Our self driving project isnt fast, but atleast can go on forever. Driving using a single neural network on a raspberry PI #donkeycar pic.twitter.com/6y5sf8AnMi . &mdash; Simen Eide (@simeneide) October 28, 2017 How my #donkeycar is detecting road, grass and horizon for #autonomousdriving. Slow and steady progress.. pic.twitter.com/VlL3hGmcWn . &mdash; Simen Eide (@simeneide) September 1, 2017",
            "url": "https://simeneide.github.io/blog/2018/01/03/talks-and-projects-2017.html",
            "relUrl": "/2018/01/03/talks-and-projects-2017.html",
            "date": " • Jan 3, 2018"
        }
        
    
  
    
        ,"post9": {
            "title": "Deep Nlp Rec",
            "content": "+++ date = 2017-09-11 title = “Deep NLP-based Recommenders at Finn.no” active = true tags = [“foo”] +++ . During a hackathon at FINN.no, we figured we wanted to learn more about deep NLP-models. FINN.no has a large database with ads of people trying to sell stuff (around 1 million active ads at any time), and they are categorized into a category tree with three or four layers. For example, full suspension bikes can be found under “Sport and outdoor activities” / “Bike sport” / “Full suspension bikes”. . In our daily jobs we are working on recommendations. There, we already have a content based (tf-idf) recommender build on Solr’s More Like This. It seems to work well in areas where our collaborative filtering approaches does not. Would it be possible to build a deep learning NLP-model of similar performance? . To achieve a measure of similarity, building a classifier of the previously mentioned categories seemed like a good choice, since we already had a lot of pre-existing data. The NLP team at Schibsted had already tokenized around six million ads as well as trained a word2vec model for us - we were ready to roll! . Some preprocessing still had to be done. We ran through all ads, concatenated the title and description strings, and after a quick look at the data took the first 15 words of each ad. . Model architecture proposed by the paper Our initial experiments were done with a simple “Bag of words” model included in the Keras repository, but we promptly switched over to “Convolutional Neural Networks for Sentence Classification” based architecture after hearing about it from our colleague, Tobias. By looking at the first 15 words of the ad, and using 200 dimensional embeddings for each word, our input is transformed into a 15x200 matrix. We apply three different convolutions on each document. The three convolutions looks at 2, 3 and 4 words (kernel sizes) in each convolution. It then max-pools each over the whole document, so that you end up with one value per document per convolution. For each kernel size you do 100 different filters. Finally you add a dense layer for classification. In addition to the standard model described in the paper, we experimented with different kernel sizes, number of filters, dense layers, batch normalization and dropout. We also added several losses, so that the model optimized both the higher and lower category at the same time. That helped. . Keras representation of our NLP model So how did it go? Our hackathon model managed to categorize 10’000 ads into 359 categories with an accuracy of 50%. We were surprised it worked so well, it was about the same accuracy image models have achieved on roughly the same ads. After tuning and pruning it further and adding 1 million data points, we have reached an accuracy of 70% on the 359 classes. In comparison, the bag-of-words model we started with managed an accuracy rate of around 25% and image recognition models have reached accuracies around 50%. . Using the model in recommendations . It is usually the case that category-similarity translates decently to ad-similarity. Using our classifier model we can serve users more ads similar to what they’re already seeing, based on the text of a selected ad. . We use the model by cutting the last dense layer (called feature layer in figure above), then comparing normalized dot products (cosine similarity) between objects. Since our our benchmarks for judging anything a success or failure is based on how it performs in a production environment, we went ahead and did that. This gave us decent results using only text, performing about 5-6% percent worse than our top collaborative filtering approach. When we made an ensemble model combining text and collaborative filtering we managed to improve our existing best model by about 10%. This is likely due to better supporting “cold ads”, or ads without traffic, while still retaining the accuracy of the collaborative filtering-model. . An example of a “cold ad”, where we think our NLP model does a better job at finding relevancy than the traditional collaborative filtering approach Collaborative filtering recommendations NLP Recommendations Further work . The pure text-model does not prioritize the popularity (or perhaps by proxy, how good the ad is) of the ad at all. This leads us to suspect that although users are being directed to similar ads, they could for example be missing an enticing image to make engagement likely. Seeing how the ensemble model in the end is optimized for click-rate, it likely only gives the NLP model high priority when the ad has low traffic. It would be interesting to somehow introduce this aspect into the NLP model. . We would like to eventually have a more thorough NLP representation of all our ads for other teams to build services and functionality on, and this recommender is an important first step to achieve that. . (This post was first published on tech.finn.no) . Resources .",
            "url": "https://simeneide.github.io/blog/2017/09/11/deep-nlp-rec.html",
            "relUrl": "/2017/09/11/deep-nlp-rec.html",
            "date": " • Sep 11, 2017"
        }
        
    
  
    
        ,"post10": {
            "title": "Jupyter Lab",
            "content": "+++ date = 2017-07-30 title = “Jupyter lab - First impression” active = true tags = [“foo”] +++ . Every three months or so I get really annoyed about Jupyter Notebook being so limited, and I usually spend half a day browsing alternatives like Spyder, PyCharm and Rodeo. Usually my search phrase is “Rstudio for python”, but wasting half a day or more I still end up with jupyter notebook. Although many good alternatives, the fact that you can work in the browser directly on the server makes it very simple to set up. . The last two weeks I have been testing out jupyter lab as a substitute for jupyter notebook for development work. Jupyter lab comes from the jupyter team, and is currently in their “Very early developer preview Alpha”, whatever that means. I have mainly used the notebook part of it, and that works more or less the same as jupyter notebook. Except for changing the locations of some buttons, jupyter lab does not (even in very early preview alpha) limit the use for a normal notebook user. . However, they have taken it much closer to an IDE by including tabs, window locations, a shell and a text editor. By doing that, it is actually possible to develop in a .py file and simultaneously run the code in a console. This is much closer to what my beloved Rstudio does for R, and is highly appreciated. . The main thing I feel is missing is all the keyboard shortcuts. . A Ctrl+Enter to execute a selected code block in the .py file onto the console is my main loss. | Also, I find myself reorganising the tabs all the time. Shortcuts to arrange the different tabs in different ways like ShiftIt would speed up development. | . Of course, I do not hold grudge against the team since they are only in its early preview developer alpha stage. I look forward to the time they move from “very early developer preview alpha” to just “early developer preview alpha”! .",
            "url": "https://simeneide.github.io/blog/2017/07/30/jupyter-lab.html",
            "relUrl": "/2017/07/30/jupyter-lab.html",
            "date": " • Jul 30, 2017"
        }
        
    
  
    
        ,"post11": {
            "title": "Cfexcel",
            "content": "+++ date = 2017-05-28 title = “Collaborative Filtering Recommendations in Spreadsheets” active = true tags = [“foo”] +++ . Most people have a love-hate relationship to spreadsheets. The spreadsheet format is simple and intuitive, and doing calculations becomes really easy. However, they quickly become too complicated as well. . . Jeremy Howard’s lecture explaining embeddings was a great use of Excel, and I implemented my own version of his excel-sheet using it for illustration purposes on how recommendation algorithms work. . Recommendations are everywhere: Netflix is trying to propose the most relevant movies, and Google is serving you personalised ads that are hopefully a (little less) annoying. The gold standard algorithm is collaborative filtering. The idea of collaborative filtering is to be able to find relevant items to recommend a user given what the user looked at before. . The full excel sheet can be found here (it is view-only, so make a copy to try it out!). . You start off getting your dataset. In our case it consist of four users that have clicked or not clicked on four different items: . . The goal of a collaborative filtering model is to predict this data. That is, if we give the model the user “Espen” and the item “Macbook Pro”, it should be able to predict a number close to one. . The model is parameterised by giving each user and each item two random numbers each. We call these numbers for “embeddings”: . . We say that our model predicts a click/no click on a pair of user and item by multiplying the embeddings. That is, we find the first embedding of Espen and multiplies it with the first embedding of the Macbook Pro. Then, we do the same for the second embedding. . -0.380.29 + -0.35-0.45 = 0.048 . That is pretty far from 1 (which is the number the model tries to predict). However, we have not trained the model yet, so it is pretty dumb so far. . The full excel sheet looks like this: . . We have seen column A to H already. Column I to L is just a copy of the embeddings from the users and items shown in the dataset. Column M is the prediction of the model (see that Espen has a 0.048 score for the Macbook Pro). Finally, column N tells us how far off we are at that particular prediction (0.952 for Espen’s case). . Since we want to make our predictions as close as the truth as possible (the truth is column H), we could say that we want as small errors as possible. That would be the same as saying we want the smallest Average Error possible (N13). The way we will get that is to alter these embeddings. If we can get those embeddings to be at certain values such that our average error is close to zero, we have made it! . One way to go forward is to change these numbers manually. If I change the sign of the first embedding of Espen to something positive I will get a higher prediction for the macbook pro. Also, if I change the sign of the second embedding of Kamilla I will also decrease the error. Now it looks like this: . . We have managed to decrease the average error from 0.71 to 0.64! Now, its pretty hard to do this by hand, so to help me I installed a little Add-on called “solver” in google spreadsheets. Basically what it does it to minimize cell N13 (which is where we have calculated our Average Error) by changing cells C3:D10 (that is where we have put all our embeddings (random numbers)). . . After pressing “Solve” the thing changes the embeddings and comes up with an error of 0.49! You can see it came up with better numbers so that our predictions (column M) are closer to the actual truth (column H). . . Doing this another time I actually get an average error of zero. Our model is perfectly predicting all the observations we have in the dataset, and we are done! . . The “real” collaborative filtering algorithms that operate in the wild is very similar to this one. They use more data and use more than two embeddings per user. At FINN.no, we are maybe using 200m datapoints and 100 “embedding-numbers” to make a recommendation model. Getting an error of zero is of course not normal. Real people are complicated, contradictive and noisy. . We’re also not doing it in Excel, but spark and tensorflow. Have a look at the presentation I held for Oslo Data Science Meetup if you want to know more about that. .",
            "url": "https://simeneide.github.io/blog/2017/05/28/CFexcel.html",
            "relUrl": "/2017/05/28/CFexcel.html",
            "date": " • May 28, 2017"
        }
        
    
  
    
        ,"post12": {
            "title": "Oslodatasciencepresentation",
            "content": "+++ date = 2017-03-30 title = “Presentation from Oslo Data Science Meetup” active = true tags = [“foo”] +++ . In February I talked about the recommendation models we have at FINN.no, and how we work to develop and test new models. It was exciting to be talking to so many people about technical stuff, but even more interesting to hear questions and comments you can take back to try out. . The full presentation can be downloaded here. . .",
            "url": "https://simeneide.github.io/blog/2017/03/30/osloDataSciencePresentation.html",
            "relUrl": "/2017/03/30/osloDataSciencePresentation.html",
            "date": " • Mar 30, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Biography . Mathematician with hacking skills at the Norwegian marketplace FINN.no and at the machine learning startup Arctic Datalab. Also doing a phd in statistics at University of Oslo. Working on personalization systems and other machine learning tasks using behaviour, image and text. Background from mathematics, statistics and financial modeling. . List your academic interests. . [interests] interests = [ “Deep Recommenders”, “Decision Making”, “Bandits and RL”, “Bayesian models”, ] +++ .",
          "url": "https://simeneide.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://simeneide.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}