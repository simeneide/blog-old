[
  {
    "objectID": "posts/2020-11-13-bayesian-deep-learning.html",
    "href": "posts/2020-11-13-bayesian-deep-learning.html",
    "title": "Bayesian Neural Nets and how to effectively train them with Stochastic Gradient Markov Chain Monte Carlo",
    "section": "",
    "text": "Robustness: Bayesian neural nets should, at least in theory and there has been some evidence for it, be more robust against out of sample data.\nBayesian framework can incorporate prior information into the models.\nBayesian models can present their uncertainty about a specific outcome, i.e. they can tell when they are uncertain!\n\n\n\nThis post have used the following references extensively: - Nemeth, C., & Fearnhead, P. (2019). Stochastic gradient Markov chain Monte Carlo, 1–31. - Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubon, D. B. (n.d.). Bayesian Data Analysis Third Edition. - Zhang, R., Li, C., Zhang, J., Chen, C., & Wilson, A. G. (2019). Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning, (2017)"
  },
  {
    "objectID": "posts/2020-11-13-bayesian-deep-learning.html#references",
    "href": "posts/2020-11-13-bayesian-deep-learning.html#references",
    "title": "Bayesian Neural Nets and how to effectively train them with Stochastic Gradient Markov Chain Monte Carlo",
    "section": "",
    "text": "This post have used the following references extensively: - Nemeth, C., & Fearnhead, P. (2019). Stochastic gradient Markov chain Monte Carlo, 1–31. - Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubon, D. B. (n.d.). Bayesian Data Analysis Third Edition. - Zhang, R., Li, C., Zhang, J., Chen, C., & Wilson, A. G. (2019). Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning, (2017)"
  },
  {
    "objectID": "posts/2020-11-13-bayesian-deep-learning.html#motivating-example",
    "href": "posts/2020-11-13-bayesian-deep-learning.html#motivating-example",
    "title": "Bayesian Neural Nets and how to effectively train them with Stochastic Gradient Markov Chain Monte Carlo",
    "section": "Motivating example",
    "text": "Motivating example\nThroughout the post we will use a motivating problem to show what we mean. Assume we have collected \\(N\\) data points \\((x_n, y_n), n\\le N\\) where \\(x_n \\in R^2\\) is a two dimensional vector, and \\(y_n \\in R^1\\) is a one dimensional response. We generate the data using a very simple “neural net”: \\[y_n = \\beta_0 + \\beta x_n + N(0,\\sigma)\\]\nwhere \\(\\beta_0\\), \\(\\beta\\) and \\(\\sigma\\) are the parameters of the model. We will collect all three parameters in the parameter \\(\\theta := \\{ \\beta_0, \\beta, \\sigma \\}\\) just to make it easier to write.\nWe can see that given both the input data \\(x_n\\) and the parameters \\(\\theta\\) y_n is actually normally distributed. That is what we call the likelihood function:\n\\[ P(D|\\theta) = \\prod_{n=1}^N P(y_n | \\theta, x_n) = \\prod_{n=1}^N N(\\beta_0 + \\beta x_n ,\\sigma)\\]\nLet us generate a dataset with \\(\\beta_0 = -1\\), \\(\\beta = 2.0\\) and \\(\\sigma = 0.1\\):\n\nN = 30\nbeta0 = torch.tensor([-1.0])\nbeta_true = torch.tensor([2.0])\nsigma_true = torch.tensor([0.5])\n\nX = dist.Uniform(-1,1).sample((N,))\nY = beta0 + X *beta_true + sigma_true*torch.normal(0,1, (N,))\ndata = {'x' : X,'y' : Y}\n\n(\n    alt.Chart(pd.DataFrame(data)).mark_circle(size=60)\n    .encode(\n        x='x',\n        y='y')\n    .properties(title=\"X versus Y in the real data\")\n    #.interactive()\n)\n\n\n\n\n\n\nNow, the goal in supervised learning is to find an estimate of the parameters \\(\\theta\\). Our objective is to maximize the likelihood function plus a prior belief of what the parameters could be. In this example we’ll make a really stupid belief and say that it is equally possible to find the parameters anywhere on the real line: \\(P(\\theta) \\sim 1\\). This simplifies the analysis, and we can then write the posterior as proportional to the likelihood only:\n\\[\nP(\\theta | X,Y) \\sim \\prod_{n=1}^N N(y_n | \\beta_0 + \\beta x_n ,\\sigma^2)\n\\]\nWe can implement this model in pytorch in the following way:\n\nclass BayesNet(nn.Module):\n    def __init__(self, seed = 42):\n        super().__init__()\n        torch.random.manual_seed(seed)\n        # Initialize the parameters with some random values\n        self.beta0 = nn.Parameter(torch.randn((1,)))\n        self.beta = nn.Parameter(torch.randn((1,)))\n        self.sigma = nn.Parameter(torch.tensor([1.0]) ) # this has to be positive\n    \n    def forward(self, data: dict):\n        return self.beta0 + self.beta*data['x']\n\n    def loglik(self, data):\n        # Evaluates the log of the likelihood given a set of X and Y\n        yhat = self.forward(data)\n        logprob = dist.Normal(yhat, self.sigma.abs()).log_prob(data['y'])\n        return logprob.sum()\n\n    def logprior(self):\n        # Return a scalar of 0 since we have uniform prior\n        return torch.tensor(0.0)\n    \n    def logpost(self, data):\n        return self.loglik(data) + self.logprior()\n\n\nmodel = BayesNet(seed =42)\nmodel.loglik(data)\n\ntensor(-92.5278, grad_fn=&lt;SumBackward0&gt;)"
  },
  {
    "objectID": "posts/2020-11-13-bayesian-deep-learning.html#metropolis-mcmc-algorithm",
    "href": "posts/2020-11-13-bayesian-deep-learning.html#metropolis-mcmc-algorithm",
    "title": "Bayesian Neural Nets and how to effectively train them with Stochastic Gradient Markov Chain Monte Carlo",
    "section": "Metropolis MCMC algorithm",
    "text": "Metropolis MCMC algorithm\nThe metropolis MCMC algorithms does nothing but starting in some random position \\(\\theta_0\\) and randomly tries to go in a direction (e.g. add a Normal distribution \\(\\theta_1 = \\theta_0 + N(0,1)\\)). If we improved (meaning that \\(P(\\theta | D)\\) increased) we use that new value. If it didnt improve, we may still move to that point depending on some probability.\nFormally the algorithm looks like this:\n\nInitialize a starting point \\(\\theta_0\\). This can either be random or somewhere you think is reasonable. For instance, you can start in the center of your prior distribution \\(argmax_{\\theta} P(\\theta)\\).\nFor each \\(n=1,2,3,...\\) do the following:\n\nSample a proposal \\(\\theta_*\\) using a proposal distribution: \\(\\theta_* \\sim J(\\theta_* | \\theta_{n-1})\\) (for example a multivariate Normal distribution: \\(J(\\theta_* | \\theta_{n-1}) = N(\\theta_* | \\theta_{n-1}, \\alpha I)\\))\nCompute “how much better this proposal is than the previous:\n\n\\[\nr = \\frac{P(\\theta_*|D)}{P(\\theta_{n-1}|D)}\n\\]\n\nSet\n\n\n\\[\n\\theta_n =\n\\begin{cases}\n    \\theta_* & \\text{with probability} \\text{ min}(1,r) \\\\\n    \\theta_{n-1} & \\text{if not}\n\\end{cases}\n\\]\nThere is one technical constraint on this proposal distribution \\(J\\) and that is that it should be symmetric. That means it should be just as likely to go in either direction: \\(J(\\theta_a | \\theta_b) = J(\\theta_b | \\theta_a)\\). Our example above, a multivariate normal with the previous step as mean, satisfy this requirement.\nSo why should this thing work? First, see that if we get a better set of parameter then \\(P(\\theta_*|D) &gt; P(\\theta_{n-1}|D)\\) and the ratio is above 1. Then we will always move to the new value (with probability 1)! That is comforting. If I stand somewhere on a smooth mountain and only take a step whenever that step is upwards I can be pretty certain I reach the top!\nBut what about that second term? What if we actually dont improve? Looking at the algorithm, that is the case when r is less than 1. We then have a positive possibility of moving anyways. Intuitively this is important for two reasons:\n\nWe want to find a posterior distribution at the end. If we only moved whenever we improved we would end up with a final value. Actually moving in slightly worse directions will allow our parameters to wiggle around the optimal solution a little bit. And luckily, this kind of wiggling will give us the posterior distribution!\nHaving a positive probability of moving in wrong directions also gives us a chance to avoid local minima. In fact, since our proposal distribution has a positive probability of jumping to any set of parameter values from any point, one can prove that the metropolis mcmc algorithm will find the optimum if it just runs for long enough.\n\n\nStep size aka learning rate\nGiven that we choose a gaussian proposal distribution with mean of the previous parameter set, we still need to set the covariance matrix of the distribution. In the example above it was \\(\\alpha I\\), where \\(\\alpha &gt; 0\\) and \\(I\\) is the identity matrix. Given this parameterization \\(\\alpha\\) determines how far we should try to jump in the metropolis algorithm. A large \\(\\alpha\\) will make us often jump far, whereas a small \\(\\alpha\\) will make us jump shortly. If we do short jumps we are likely to get proposal parameters that does not perform too bad and the metropolis algorithm will often accept the new parameters, but we will take very short steps each time. On the other hand, if we do large jumps we may sometiems get very good gains, but they will also be very bad very often. Therefore larger values of \\(\\alpha\\) will cause the accept probability to be low, but with large gains whenver it accepts.\nThis trade off looks very similar to learning rates in deep learning: Small steps will converge but slowly, and large steps takes giant leaps of faith and may not converge very well. The difference is of course that the metropolis algorithm will just not accept any proposals if you set \\(\\alpha\\) too high. There are more complicated algorithms that tries to auto-set \\(\\alpha\\), but for now we will just find a reasonable value (by trial and error) and stick to it."
  },
  {
    "objectID": "posts/2020-11-13-bayesian-deep-learning.html#implementation-of-metropolis-algorithm",
    "href": "posts/2020-11-13-bayesian-deep-learning.html#implementation-of-metropolis-algorithm",
    "title": "Bayesian Neural Nets and how to effectively train them with Stochastic Gradient Markov Chain Monte Carlo",
    "section": "Implementation of Metropolis algorithm",
    "text": "Implementation of Metropolis algorithm\nLet us implement a small “optimizer” that finds the posterior distribution using the metropolis algorithm. The main component of this class is the step() function, and it goes through the steps above. In addition we have implemented a fit() function for the training loop and a dictionary that holds all the parameters values over iterations so we can look at them later:\n\nclass MetropolisOptimizer():\n    def __init__(self, net, alpha):\n        super().__init__()\n        self.net = net\n        self.alpha = alpha\n    \n    @torch.no_grad()\n    def step(self, data=None):\n        # Step 1:\n        proposal_net = copy.deepcopy(self.net) # we copy the whole network instead of just the parameters for simplicity\n        for name, par in proposal_net.named_parameters():\n            newpar = par + torch.normal(torch.zeros_like(par), self.alpha)\n            par.copy_(newpar)\n        \n        # Step 2: calculate ratio\n        ratio = torch.exp(proposal_net.logpost(data) - self.net.logpost(data))\n\n        # Step 3: update with some probability:\n        if (random.random()&lt;ratio).bool():\n            self.net = proposal_net\n        else:\n            pass\n        return self.net\n\n    def fit(self, data=None, num_steps=1000):\n        # We create one tensor per parameter so that we can keep track of the parameter values over time:\n        self.parameter_trace = {\n            key : torch.zeros( (num_steps,) + par.size()) for key, par in self.net.named_parameters()}\n\n        for s in range(num_steps):\n            current_net = self.step(data)\n            for key, val in current_net.named_parameters():\n                self.parameter_trace[key][s,] = val.data\n\nmodel = BayesNet()\ntrainer = MetropolisOptimizer(model, alpha=0.02)\n# Train the model (and take the time):\n%time trainer.fit(data, num_steps=5000)\n\nCPU times: user 2.41 s, sys: 23 ms, total: 2.43 s\nWall time: 2.68 s\n\n\nWe only have 3 parameters and can visualize all of them. We see that all three variables first trods its way towards the area were its “supposed to be” and then starts wiggling around that area. This is the posterior distribution! In addition, we visualize a two dimensional plot of where \\(\\beta_0\\) and \\(\\beta\\) are over all steps:\n\n\n\n\n\n\n\nAll parameters starts in some random position and then quickly iterates itself towards where the posterior distributions should be. In the last plot we can see this over the two parameters \\(\\beta\\) and \\(\\beta_0\\) where they start in the lower right area and end up around there posteiror distribution in the upper left region.\nThe iterations before the model has converged to its posterior distribution (happens at around step 1500 or so) is called the burn-in phase, or it would just be called “training” in a standard deep learning setting. However, in bayesian deep learning, this is where the fun starts, because now the mcmc starts to describe the full posterior distribution. These two phases are very clear in the lower right plot!"
  },
  {
    "objectID": "posts/2020-11-13-bayesian-deep-learning.html#implementation-of-the-sgld-algorithm",
    "href": "posts/2020-11-13-bayesian-deep-learning.html#implementation-of-the-sgld-algorithm",
    "title": "Bayesian Neural Nets and how to effectively train them with Stochastic Gradient Markov Chain Monte Carlo",
    "section": "Implementation of the SGLD algorithm",
    "text": "Implementation of the SGLD algorithm\nThe implementation of SGD algorithm requires us the calculate the gradient of the likelihood function. That is something deep learning frameworks are very good at. On the other hand, the SGLD algorithm will always accept the proposed parameters, making the optimization algorithm much simpler than the previous metropolis algorithm (at the risk of being a bad approximation to the posterior). For this implementation, note that we do not subsample the data, as we only have 30 anyways.\n\nfrom torch.optim.optimizer import Optimizer\nclass TrainOptimizerSGLD(Optimizer):\n    def __init__(self, net, alpha=1e-4):\n        super(TrainOptimizerSGLD, self).__init__(net.parameters(), {})\n        self.net = net\n        self.alpha = alpha\n    \n    def step(self, batch, batch_size=1, num_data=1):\n        self.zero_grad()\n        weight = num_data/batch_size\n        loss = -self.net.loglik(batch)*weight - self.net.logprior()\n        loss.backward()\n        \n        with torch.no_grad():\n            for name, par in self.net.named_parameters():\n                newpar = (\n                    par - 0.5*self.alpha*par.grad \n                    + torch.normal(torch.zeros_like(par), std=self.alpha)\n                )\n                par.copy_(newpar)\n        return loss\n\n    def fit(self, data=None, num_steps=1000):\n        # We create one tensor per parameter so that we can keep track of the parameter values over time:\n        self.parameter_trace = {key : torch.zeros( (num_steps,) + par.size()) for key, par in self.net.named_parameters()}\n\n        for s in range(num_steps):\n            loss = self.step(data)\n            for key, val in self.net.named_parameters():\n                self.parameter_trace[key][s,] = val.data\n\nmodel = BayesNet()\ntrainer = TrainOptimizerSGLD(model, alpha=0.04)\n# Train the model (and take the time):\n%time trainer.fit(data, num_steps=5000)\nplot_parameter_trace(trainer)\n\nCPU times: user 2.76 s, sys: 68.4 ms, total: 2.83 s\nWall time: 2.92 s\n\n\n\n\n\n\n\nCompared to the metropolis algorithm we see that the beta parameters quickly snap into place. However, the sigma parameter is varying much more than anticipated. This must be due to the approximation error in the discretization of the stochastic differential equation. Reducing step size should solve this problem:\n\nmodel = BayesNet()\ntrainer = TrainOptimizerSGLD(model, alpha=0.005)\n# Train the model (and take the time):\n%time trainer.fit(data, num_steps=5000)\nplot_parameter_trace(trainer)\n\nCPU times: user 2.88 s, sys: 82.9 ms, total: 2.96 s\nWall time: 3.13 s\n\n\n\n\n\n\n\nVoila! Compared to the Metropolis algorithm, the SGLD snapped the \\(beta\\) parameters pretty much straight into the correct area. It shows a great speed improvement from the random walk algorithm of Metropolis, something which is cruical for more complicated problems. At the same time, we can see that \\(\\sigma\\) never really converged to its true value around 0.5, but keeps jumping up. This must be due to the Euler approximation in the Langevin diffusion, and should be solveable by reducing step sizes. Let us repeat the SGLD algorithm with a smaller step size. We see that although the convergence is slightly slower (but much faster than the metropolis!), we end up with a better approximation:"
  },
  {
    "objectID": "posts/2020-11-13-bayesian-deep-learning.html#comparing-csgmcmc-vs-sgld-vs-sgd",
    "href": "posts/2020-11-13-bayesian-deep-learning.html#comparing-csgmcmc-vs-sgld-vs-sgd",
    "title": "Bayesian Neural Nets and how to effectively train them with Stochastic Gradient Markov Chain Monte Carlo",
    "section": "Comparing cSGMCMC vs SGLD vs SGD",
    "text": "Comparing cSGMCMC vs SGLD vs SGD\nGiven our optimizer above it is easy to modify it to run different algorithms, and we will compare these three algorithms in a simple run.\n\nconfigs = {\n    'cSGMCMC-temp' : {\n        'max_step' : 30e3,\n        'noise' : True,\n        'noise_part2' : True\n    },\n    'cSGMCMC' : {\n        'max_step' : 30e3,\n        'noise' : True\n    },\n    'SGLD' : {\n        'max_step' : 1564*100,\n        'noise' : True\n    },\n    'SGD' : {\n        'max_step' : 1564*100,\n        'noise' : False\n    },\n}\n\nfor name, config in configs.items():\n    model = LitMNIST(lr_max=1e-3, lr_min=1e-5, **config)\n    logger = TensorBoardLogger(\"logs\", name=name)\n    trainer = Trainer(gpus=1, progress_bar_refresh_rate=0, logger=logger, max_epochs=100)\n    trainer.fit(model, dataset)\n    print(f\"Finished training {name}\")\n\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n  | Name      | Type             | Params\n-----------------------------------------------\n0 | layer_1   | Conv2d           | 448   \n1 | layer_2   | Conv2d           | 4 K   \n2 | linear1   | Linear           | 147 K \n3 | linear2   | Linear           | 1 K   \n4 | criterion | CrossEntropyLoss | 0     \n5 | train_acc | Accuracy         | 0     \n6 | val_acc   | Accuracy         | 0     \nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type             | Params\n-----------------------------------------------\n0 | layer_1   | Conv2d           | 448   \n1 | layer_2   | Conv2d           | 4 K   \n2 | linear1   | Linear           | 147 K \n3 | linear2   | Linear           | 1 K   \n4 | criterion | CrossEntropyLoss | 0     \n5 | train_acc | Accuracy         | 0     \n6 | val_acc   | Accuracy         | 0     \nFinished training cSGMCMC\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type             | Params\n-----------------------------------------------\n0 | layer_1   | Conv2d           | 448   \n1 | layer_2   | Conv2d           | 4 K   \n2 | linear1   | Linear           | 147 K \n3 | linear2   | Linear           | 1 K   \n4 | criterion | CrossEntropyLoss | 0     \n5 | train_acc | Accuracy         | 0     \n6 | val_acc   | Accuracy         | 0     \nFinished training SGLD\nFinished training SGD"
  },
  {
    "objectID": "posts/2020-11-13-bayesian-deep-learning.html#results",
    "href": "posts/2020-11-13-bayesian-deep-learning.html#results",
    "title": "Bayesian Neural Nets and how to effectively train them with Stochastic Gradient Markov Chain Monte Carlo",
    "section": "Results",
    "text": "Results\n\n\n\nIllustration of how to implement the algorithms, unfortunately not sufficient time to evaluate the different optimization algorithms properly. Some findings:\n\nWe would in this case expect that SGD performs better than the alternatives as SGD is finding the maximum aposterior \\(\\theta_{MAX} = argmax_{\\theta} P(D|\\theta)\\), and the others are find a posterior distribution “around” that.\n\nSeems confirmed by comparing SGD and SGLD.\n\nWe see that cSGLD is oscilating, each of the optimas should, according to the cSGLD paper be different modes of the distribution. If we were to estimate the loss by ensembling the whole distribution, the test loss should be lower than for SGD and SGLD.\n\nWe did not have time for this."
  },
  {
    "objectID": "posts/2020-09-14-colab-vscode-gpu.html",
    "href": "posts/2020-09-14-colab-vscode-gpu.html",
    "title": "Running Google Colab with VS code",
    "section": "",
    "text": "Running VScode and the python extension is great for development. I get clean python files and can run my code interactively. It is the same setup we have at work and I then remotely connect to a server with more cpu and GPU. However, for my side gigs I havent really figured out a workflow, until now.\nThis morning I found colab-ssh. It enables you to remotely connect your google colab instance to your local VScode! And if you have a GPU runtime on google colab you get that as well, of course. Combining this with a small google drive mounting, and I get more or less my local working environment but with GPU acceleration.\n\nA quick step-by-step (see colab-ssh for updates if this doesnt work): 1. Open a new google colab notebook. 2. In first cell: Mount your google drive using these commands (you need to follow link and authorize):\nfrom google.colab import drive\ndrive.mount('/root/gdrive')\n\nGo to this site and get an ngrok token.\nSecond cell: Add your token and create a password. Then add+run this:\n\n# Install colab_ssh on google colab\n!pip install colab_ssh --upgrade\nngrokToken = 'XXX'\npassword = 'XXX'\nfrom colab_ssh import launch_ssh, init_git\nlaunch_ssh(ngrokToken,password)\n\nYou’ll now see something like this:\n\nCollecting colab_ssh\n  Downloading https://files.pythonhosted.org/packages/a7/c5/eedfd8b374fead9d863cb7031d9dc97fed50003372922ba0efd85d9fe3e0/colab_ssh-0.2.63-py3-none-any.whl\nInstalling collected packages: colab-ssh\nSuccessfully installed colab-ssh-0.2.63\nSuccessfully running 2.tcp.ngrok.io:13254\n[Optional] You can also connect with VSCode SSH Remote extension using this configuration:\n\n    Host google_colab_ssh\n        HostName 2.tcp.ngrok.io\n        User root\n        Port 12345\n\nGo to your local VScode and select Remote-SSH: Open Configuration File, and paste the config above:\n\n\n\n\nSelect Remote-SSH: Connect to Host and select the google colab ssh connection.\n\n\nVoila! Up and running with gpu and your google drive attached.\nUnfortunately, the hostname and port changes each time, and you still have to manually open the google colab.\nStill, really great work by Wassim Benzarti."
  },
  {
    "objectID": "posts/2021-05-05-dynamic-slate-recsys.html",
    "href": "posts/2021-05-05-dynamic-slate-recsys.html",
    "title": "Dynamic Slate Recommendation with Gated Recurrent Units and Thompson Sampling",
    "section": "",
    "text": "Authors: Simen Eide, David S. Leslie, Arnoldo Frigessi Publication: Data Mining and Knowledge Discovery Preprint: 5 May 2021 Publication: 19 July 2022\nPaper link: https://link.springer.com/content/pdf/10.1007/s10618-022-00849-w.pdf\nCode link: https://github.com/finn-no/recsys-slates-dataset"
  },
  {
    "objectID": "posts/2021-05-05-dynamic-slate-recsys.html#abstract",
    "href": "posts/2021-05-05-dynamic-slate-recsys.html#abstract",
    "title": "Dynamic Slate Recommendation with Gated Recurrent Units and Thompson Sampling",
    "section": "Abstract",
    "text": "Abstract\nWe consider the problem of recommending relevant content to users of an internet platform in the form of lists of items, called slates. We introduce a variational Bayesian Recurrent Neural Net recommender system that acts on time series of interactions between the internet platform and the user, and which scales to real world industrial situations. The recommender system is tested both online on real users, and on an offline dataset collected from a Norwegian web-based marketplace, this http URL, that is made public for research. This is one of the first publicly available datasets which includes all the slates that are presented to users as well as which items (if any) in the slates were clicked on. Such a data set allows us to move beyond the common assumption that implicitly assumes that users are considering all possible items at each interaction. Instead we build our likelihood using the items that are actually in the slate, and evaluate the strengths and weaknesses of both approaches theoretically and in experiments. We also introduce a hierarchical prior for the item parameters based on group memberships. Both item parameters and user preferences are learned probabilistically. Furthermore, we combine our model with bandit strategies to ensure learning, and introduce `in-slate Thompson Sampling’ which makes use of the slates to maximise explorative opportunities. We show experimentally that explorative recommender strategies perform on par or above their greedy counterparts. Even without making use of exploration to learn more effectively, click rates increase simply because of improved diversity in the recommended slates."
  },
  {
    "objectID": "posts/2018-10-06-recsys2018.html",
    "href": "posts/2018-10-06-recsys2018.html",
    "title": "Takeaways Recsys 2018",
    "section": "",
    "text": "For the first time Ive actually made a summary of all the papers and presentations I found noteworthy at a conference (allright, there were more, but this is a start). Below is my notes, with links etc. The purpose of the notes is mainly for myself to remember and revisit what I found interesting, but I see no reasons not to share to others. Does not include my own [paper]( ).\n\nKeynote dlrs - Joachims: Deep learning with logged bandit feedback\n\nPaper: http://www.cs.cornell.edu/people/tj/publications/joachims_etal_18a.pdf\nidea: utilize current policy to build a better contextual bandits to recommend.\nusing Inverse Propensity Scoring\nUsing Self-normalizing IPS estimator (SNIPS)\nAlso using self normalizing\n\n\n\n\nA very similar talk seem to be posted here:\n\n\n\n\nYoutube, Minmin Chen: Off-policy correction for a REINFORCE Rec system\n\nInvited talk on the evaluation workshop REVEAL\nYoutube shared their work on how the models they used now works, focus on evaluation\nMaximized recommendations based on expected future reward\nthey do not use td to train the model, rather the all the actual future rewards and discounts them to get training signal.\n\n\n\n\nCorrects off-policy updates with importance sampling/IPS \nExploration is costly, so they tune ut with an extra weight that they set.\nRecommends a set of top K items. Assume additive rewards: \n\n\n\nResults\n\nImproved recs, also able to recommend more items into the tail.\n\n\n\n\n\nRecsys-competition winners: Two-stage model for Automatic Playlist Continuation at Scale\n\nChallenge website: http://www.recsyschallenge.com/2018/\nWinner paper: http://www.cs.toronto.edu/~mvolkovs/recsys2018_challenge.pdf\nTask was to complete a user playlist at spotify.\nA playlist may have be created over a long time.\nGiven the first items, predict the last ones.\n\nApproach:\n\nstep 1: Reduce candidate set of all items to 20k by using a temporal convolutional layer.\n\n(lstm worked too, but was slower to train and iterate on).\nThe step 1 was really about maximizing recall.\n\nStep 2: xgboost classifier on these candidates\n\n\n\n\nCategorical-attributes-based item classification for recommender systems: hiarchical softmax and multi loss\n\nPaper: https://dl.acm.org/citation.cfm?id=3240367\nSetting: Next item prediction with items within some category structure.\nUsing negative sampling during training\nDoes the recommender improve by predicting a hiarchical softmax instead of doing multi target prediction?\nResult: Using hiarchical modeling is better than multi target. Testet with MAP@5 on recsys16 dataset and “large propertary dataset”.\nAlso helps with cold start\n\nOwn comments: Unsure of the improvement is due to negative sampling or that you infer more structure in your data/model.\n\n\n\"Categorical-Attributes-Based Item Classification for Recommender Systems\" by @QianZhao3 and Google folks lead by @edchi - really interesting idea: categorical labels as outputs of multitask model you are optimizing when recommending items #RecSys2018 https://t.co/Bkljw0zVFf pic.twitter.com/PavBxdtLaX\n\n— Xavier @ #recsys2018🎗🤖🏃 (@xamat) October 5, 2018\n\n\n\n\n\n\nKeynote 2 dlrs - Ray Jiang, deepmind: slate recommendation (part 1)\n\nRelevant paper (seems unpublished): https://arxiv.org/pdf/1803.01682.pdf\npredict a full feed instead of single items\nuse a VAE to do this,\n“Works really well”.\nTested on Recsys 2015: was the best slate dataset they could find\n\n\n\nhttps://arxiv.org/abs/1803.01682\n\n\nCalibrated Recommendations\n\nPaper: https://dl.acm.org/citation.cfm?id=3240372\nIf you have seen 70% drama and 30% horror, optimizing a recommender on precision, the best solution is to give you 100% drama and get 70% precision.\nThe paper suggests to calibrate the recommendations to be more representative.\nDone by regularizing the recommendations with the KL divergence of categories (genres in this case)\nDone as a post processing step.\nResult: Can rerank top recommendations to a much more representative distribution without losing accuracy.\n\nhttps://dl.acm.org/citation.cfm?id=3240372\n\n\nExplore, Exploit, and Explain: Personalizing Explainable Recommendations with Bandits\n\nPaper: https://dl.acm.org/citation.cfm?id=3240354\nFeed-bandit that uses a factorization machine to predict and explain recommendations\ncontext: Home page of spotify account. Different shelves of recommendations, each with an explanation (“because you recently listened to..”)\n\n\n\nInvited talk Netflix: Correlation & Causation\n\nYves Raimond, AI director Netflix\nNetflix’s approach: personalize everything\nMade some thoughts about the do operator P(Y | do(X))\nMaybe better for recs to focus on P(click | do(X)) - P(click | not do X)\nIPS:\npro: simple, model-agnostic\ncon: only unbiased if no unobserved cofounders, high variance\nAlternative til IPS: Instrumental Variable\nused in econometrics\npro: robust to unobserved cofounders\ncon: bias/var depends on strength of IV, hard to scale\n\n \n\n\n\nGeneration meets recommendation - “Generating new items that fit most users”\n\nPaper: https://arxiv.org/abs/1808.01199\n\n\n\n“Consider a movie studio aiming to produce a set of new movies for summer release: What types of movies it should produce? Who would the movies appeal to?”\n“Specifically, we leverage the latent space obtained by training a deep generative model—the Variational Autoencoder (VAE)—via a loss function that incorporates both rating performance and item reconstruction terms.”\n“We then apply a greedy search algorithm that utilizes this learned latent space to jointly obtain K plausible new items, and user groups that would find the items appealing.”\n\n\n\n\nOn the robustness and discriminative power of information retrieval metrics for top-N recommendation\n\nPaper: https://dl.acm.org/authorize.cfm?key=N668684\nAn evaluation of robustness of many offline metrics at different ranking level. E.g. MRR@5, Recall@10, MAP@100, …\nTakeaway 1: Use a high cutoff (e.g. 100 instead of 10) when doing offline evaluations, like MRR. The metric is more robust, and highly correlated to the MRR@10 values\nTakeaway 2: MRR is one of the lesser robust offline metrics.\n\n\nTakeaway 2: mrr er ganske lite robust.\n\n\nUnbiased offline recommender evaluation for missing-not-at-random implicit feedback\n\nImplicit-feedback Recommenders (ImplicitRec) leverage positive only user-item interactions, such as clicks, to learn personalized user preferences. Recommenders are often evaluated and compared offline using datasets collected from online platforms. These platforms are subject to popularity bias (i.e., popular items are more likely to be presented and interacted with), and therefore logged ground truth data are Missing-Not-At-Random (MNAR).\n\n\n“Average over all” estimators are biased in Implicit rec datasets\nUse IPS to evaluate policies.\nreduce bias with 30% in a yahoo! music datset.\nPaper: https://dl.acm.org/citation.cfm?id=3240355\n\n\n\nRecoGym\n\nSimulation environment where you can evaluate your recommender agent\nFollows the same style as openAI gym: env.step(action)\nWhen we tried it a bit the day before, the users seemed to click on the same items over and over again, probably some tuning that needs to be done there?\nThis is sort of an alternative approach to offline evaluation. Simulators are limited by their generating model, but can we still use it to test algorithms for convergence etc?\nUnrelated to talk and recogym, but some notes me and Olav did on rec simulations during conf. Same ideas:\n\n\n\n\nNews session-based recommendations using DNN\n\nA recommendation algorithm to recommend news.\nFreshness and coldstart big problems.\nSeparate item representation that uses a lot of content, independent of users\nUnfortunately not tested in prod (authors from large news corp.)\nTested on offline data: Beats everything, incl gru4rec ++\nPaper: https://arxiv.org/abs/1808.00720\nCode: https://github.com/criteo-research/reco-gym\n\n\n\n\nWhat happens if users only share last n days of data? (Exploring recommendations under user-controlled data filtering)\n\n\n“Using the MovieLens dataset as a testbed, we evaluated three widely used collaborative filtering algorithms.”\n“Our experiments demonstrate that filtering out historical user data does not significantly affect the overall recommendation performance.”\nImpacts those who opted out (naturally)\n\n\nPaper: https://scholar.google.com/citations?user=Vyj2jeoAAAAJ&hl=en#d=gs_md_cita-d&p=&u=%2Fcitations%3Fview_op%3Dview_citation%26hl%3Den%26user%3DVyj2jeoAAAAJ%26citation_for_view%3DVyj2jeoAAAAJ%3A2osOgNQ5qMEC%26tzom%3D420\n\n\nInteractive recommendation via deep neural memory augmented contextual bandits\n\ncreated a recsys simulator? check out…\nPaper: https://dl.acm.org/citation.cfm?id=3240344"
  },
  {
    "objectID": "posts/2020-06-30-pytorch-raspberry.html",
    "href": "posts/2020-06-30-pytorch-raspberry.html",
    "title": "Installing Pytorch on a raspberry pi 4",
    "section": "",
    "text": "I found these wheel builds from Thomas Viehmann that worked very well on a rpi4 64 bit running python 3.7. They are pytorch 1.6.0 and avoids the original hacks. Only issue was that my camera stopped working, but manage to circumvent it by using a different driver (v4l-utils) and using opencv’s VideoCapture() to get images. Got it running in a docker image with the following (removed some parts that I dont think is necessary):\n(the balena docker image is a fairly stripped down ubuntu image)\nFROM balenalib/raspberrypi4-64:latest\n# Defines our working directory in container\nWORKDIR /usr/src/app\nRUN sudo apt-get update\nRUN apt-get install -y gcc python3-dev v4l-utils python3-opencv python3-pip python3-setuptools libffi-dev libssl-dev\n\n# PYTORCH: \nRUN wget https://mathinf.com/pytorch/arm64/torch-1.6.0a0+b31f58d-cp37-cp37m-linux_aarch64.whl\nRUN wget https://mathinf.com/pytorch/arm64/torchvision-0.7.0a0+78ed10c-cp37-cp37m-linux_aarch64.whl\nRUN sudo apt-get install -y python3-numpy python3-wheel python3-setuptools python3-future python3-yaml python3-six python3-requests python3-pip python3-pillow\nRUN pip3 install torch*.whl torchvision*.whl"
  },
  {
    "objectID": "posts/2020-06-30-pytorch-raspberry.html#step-by-step",
    "href": "posts/2020-06-30-pytorch-raspberry.html#step-by-step",
    "title": "Installing Pytorch on a raspberry pi 4",
    "section": "Step-by-step:",
    "text": "Step-by-step:\n\nPIP install pytorch from wheel\nDownload wheel from here https://github.com/nmilosev/pytorch-arm-builds and run sudo pip3 install torch-1.1.0-cp37-cp37m-linux_armv7l.whl\n\n\nRename some files\nThen if you try to run sudo python3 -c \"import torch\" you get:\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/local/lib/python3.7/dist-packages/torch/__init__.py\", line 79, in &lt;module&gt;\n    from torch._C import *\nModuleNotFoundError: No module named 'torch._C'\nCan be fixed by the following:\ncd /usr/local/lib/python3.7/dist-packages/torch\nsudo mv _C.cpython-37m-arm-linux-gnueabi.so _C.so\nsudo mv _dl.cpython-37m-arm-linux-gnueabi.so _dl.so"
  },
  {
    "objectID": "posts/2017-03-30-osloDataSciencePresentation.html",
    "href": "posts/2017-03-30-osloDataSciencePresentation.html",
    "title": "Presentation from Oslo Data Science Meetup",
    "section": "",
    "text": "In February I talked about the recommendation models we have at FINN.no, and how we work to develop and test new models. It was exciting to be talking to so many people about technical stuff, but even more interesting to hear questions and comments you can take back to try out.\nThe full presentation can be downloaded here.\n\n\n\nTensorflow-spark-combination"
  },
  {
    "objectID": "posts/2017-05-28-CFexcel.html",
    "href": "posts/2017-05-28-CFexcel.html",
    "title": "Collaborative Filtering Recommendations in Spreadsheets",
    "section": "",
    "text": "Most people have a love-hate relationship to spreadsheets. The spreadsheet format is simple and intuitive, and doing calculations becomes really easy. However, they quickly become too complicated as well.\n\n\n\nAlgorithms\n\n\nJeremy Howard’s lecture explaining embeddings was a great use of Excel, and I implemented my own version of his excel-sheet using it for illustration purposes on how recommendation algorithms work.\nRecommendations are everywhere: Netflix is trying to propose the most relevant movies, and Google is serving you personalised ads that are hopefully a (little less) annoying. The gold standard algorithm is collaborative filtering. The idea of collaborative filtering is to be able to find relevant items to recommend a user given what the user looked at before.\nThe full excel sheet can be found here (it is view-only, so make a copy to try it out!).\nYou start off getting your dataset. In our case it consist of four users that have clicked or not clicked on four different items:\n\n\n\ndataset\n\n\nThe goal of a collaborative filtering model is to predict this data. That is, if we give the model the user “Espen” and the item “Macbook Pro”, it should be able to predict a number close to one.\nThe model is parameterised by giving each user and each item two random numbers each. We call these numbers for “embeddings”:\n\n\n\nembeddings\n\n\nWe say that our model predicts a click/no click on a pair of user and item by multiplying the embeddings. That is, we find the first embedding of Espen and multiplies it with the first embedding of the Macbook Pro. Then, we do the same for the second embedding.\n-0.380.29 + -0.35-0.45 = 0.048\nThat is pretty far from 1 (which is the number the model tries to predict). However, we have not trained the model yet, so it is pretty dumb so far.\nThe full excel sheet looks like this:\n\n\n\ncf-untrained\n\n\nWe have seen column A to H already. Column I to L is just a copy of the embeddings from the users and items shown in the dataset. Column M is the prediction of the model (see that Espen has a 0.048 score for the Macbook Pro). Finally, column N tells us how far off we are at that particular prediction (0.952 for Espen’s case).\nSince we want to make our predictions as close as the truth as possible (the truth is column H), we could say that we want as small errors as possible. That would be the same as saying we want the smallest Average Error possible (N13). The way we will get that is to alter these embeddings. If we can get those embeddings to be at certain values such that our average error is close to zero, we have made it!\nOne way to go forward is to change these numbers manually. If I change the sign of the first embedding of Espen to something positive I will get a higher prediction for the macbook pro. Also, if I change the sign of the second embedding of Kamilla I will also decrease the error. Now it looks like this:\n\n\n\ncf-manual-optimzation\n\n\nWe have managed to decrease the average error from 0.71 to 0.64! Now, its pretty hard to do this by hand, so to help me I installed a little Add-on called “solver” in google spreadsheets. Basically what it does it to minimize cell N13 (which is where we have calculated our Average Error) by changing cells C3:D10 (that is where we have put all our embeddings (random numbers)).\n\n\n\nsolver\n\n\nAfter pressing “Solve” the thing changes the embeddings and comes up with an error of 0.49! You can see it came up with better numbers so that our predictions (column M) are closer to the actual truth (column H).\n\n\n\nsolver-step\n\n\nDoing this another time I actually get an average error of zero. Our model is perfectly predicting all the observations we have in the dataset, and we are done!\n\n\n\nsolver-solved\n\n\nThe “real” collaborative filtering algorithms that operate in the wild is very similar to this one. They use more data and use more than two embeddings per user. At FINN.no, we are maybe using 200m datapoints and 100 “embedding-numbers” to make a recommendation model. Getting an error of zero is of course not normal. Real people are complicated, contradictive and noisy.\nWe’re also not doing it in Excel, but spark and tensorflow. Have a look at the presentation I held for Oslo Data Science Meetup if you want to know more about that."
  },
  {
    "objectID": "posts/2017-09-11-deep-nlp-rec.html",
    "href": "posts/2017-09-11-deep-nlp-rec.html",
    "title": "Deep NLP-based Recommenders at Finn.no",
    "section": "",
    "text": "During a hackathon at FINN.no, we figured we wanted to learn more about deep NLP-models. FINN.no has a large database with ads of people trying to sell stuff (around 1 million active ads at any time), and they are categorized into a category tree with three or four layers. For example, full suspension bikes can be found under “Sport and outdoor activities” / “Bike sport” / “Full suspension bikes”.\nIn our daily jobs we are working on recommendations. There, we already have a content based (tf-idf) recommender build on Solr’s More Like This. It seems to work well in areas where our collaborative filtering approaches does not. Would it be possible to build a deep learning NLP-model of similar performance?\nTo achieve a measure of similarity, building a classifier of the previously mentioned categories seemed like a good choice, since we already had a lot of pre-existing data. The NLP team at Schibsted had already tokenized around six million ads as well as trained a word2vec model for us - we were ready to roll!\nSome preprocessing still had to be done. We ran through all ads, concatenated the title and description strings, and after a quick look at the data took the first 15 words of each ad.\nOur initial experiments were done with a simple “Bag of words” model included in the Keras repository, but we promptly switched over to “Convolutional Neural Networks for Sentence Classification” based architecture after hearing about it from our colleague, Tobias. By looking at the first 15 words of the ad, and using 200 dimensional embeddings for each word, our input is transformed into a 15x200 matrix. We apply three different convolutions on each document. The three convolutions looks at 2, 3 and 4 words (kernel sizes) in each convolution. It then max-pools each over the whole document, so that you end up with one value per document per convolution. For each kernel size you do 100 different filters. Finally you add a dense layer for classification. In addition to the standard model described in the paper, we experimented with different kernel sizes, number of filters, dense layers, batch normalization and dropout. We also added several losses, so that the model optimized both the higher and lower category at the same time. That helped.\nSo how did it go? Our hackathon model managed to categorize 10’000 ads into 359 categories with an accuracy of 50%. We were surprised it worked so well, it was about the same accuracy image models have achieved on roughly the same ads. After tuning and pruning it further and adding 1 million data points, we have reached an accuracy of 70% on the 359 classes. In comparison, the bag-of-words model we started with managed an accuracy rate of around 25% and image recognition models have reached accuracies around 50%."
  },
  {
    "objectID": "posts/2017-09-11-deep-nlp-rec.html#using-the-model-in-recommendations",
    "href": "posts/2017-09-11-deep-nlp-rec.html#using-the-model-in-recommendations",
    "title": "Deep NLP-based Recommenders at Finn.no",
    "section": "Using the model in recommendations",
    "text": "Using the model in recommendations\nIt is usually the case that category-similarity translates decently to ad-similarity. Using our classifier model we can serve users more ads similar to what they’re already seeing, based on the text of a selected ad.\nWe use the model by cutting the last dense layer (called feature layer in figure above), then comparing normalized dot products (cosine similarity) between objects. Since our our benchmarks for judging anything a success or failure is based on how it performs in a production environment, we went ahead and did that. This gave us decent results using only text, performing about 5-6% percent worse than our top collaborative filtering approach. When we made an ensemble model combining text and collaborative filtering we managed to improve our existing best model by about 10%. This is likely due to better supporting “cold ads”, or ads without traffic, while still retaining the accuracy of the collaborative filtering-model.\n\n\n\nAn example of a “cold ad”, where we think our NLP model does a better job at finding relevancy than the traditional collaborative filtering approach\n\n\n\n\n\nCollaborative filtering recommendations\n\n\n\n\n\nNLP Recommendations"
  },
  {
    "objectID": "posts/2017-09-11-deep-nlp-rec.html#further-work",
    "href": "posts/2017-09-11-deep-nlp-rec.html#further-work",
    "title": "Deep NLP-based Recommenders at Finn.no",
    "section": "Further work",
    "text": "Further work\nThe pure text-model does not prioritize the popularity (or perhaps by proxy, how good the ad is) of the ad at all. This leads us to suspect that although users are being directed to similar ads, they could for example be missing an enticing image to make engagement likely. Seeing how the ensemble model in the end is optimized for click-rate, it likely only gives the NLP model high priority when the ad has low traffic. It would be interesting to somehow introduce this aspect into the NLP model.\nWe would like to eventually have a more thorough NLP representation of all our ads for other teams to build services and functionality on, and this recommender is an important first step to achieve that.\n(This post was first published on tech.finn.no)\nResources"
  },
  {
    "objectID": "posts/2019-01-17-pilco+improve.html",
    "href": "posts/2019-01-17-pilco+improve.html",
    "title": "PILCO and Deep PILCO",
    "section": "",
    "text": "Papers: - Deisenroth, M. P., & Rasmussen, C. E. (2011). PILCO: A model-based and data-efficient approach to policy search. Proceedings of the 28th International Conference on Machine Learning, ICML 2011, 465–472. - Gal, Y., Mcallister, R. T., & Rasmussen, C. E. (2016). Improving PILCO with Bayesian Neural Network Dynamics Models. Data-Efficient Machine Learning Workshop, ICML, 1–7.\nThe papers shows how to find good policies with relatively few observations on classical control problems (mountain car, pole swing up etc) using probabilistic model based reinforcement learning.\nBeing model based in reinforcement learning means that you build a statistical model of the environment, a world model. The RL algorithm can then search for an optimal policy by simulating from this world model. This is more data efficient than in the “model free” reinforcement learning algorithms, where one needs millions of examples to learn relatively simple games. However, model based simulators suffers from their approximations to the real world, which often ends up as huge errors when you simulate multiple steps. By introducing a probabilistic dynamics model, the PILCO algorithms tries to account for the fact that future trajectories are uncertain by introducing parameter uncertainty. The original PILCO paper does this by using Gaussian Processes, while the deep PILCO use a neural net with the “dropout trick” to approximate a bayesian neural net.\n\nThe framework assumes that the world model is of the form\n\\(x_t=f(x_{t-1},u_{t-1})\\)\nwhere \\(x \\in R^d\\) is a continuous state of the world at time t, \\(u_t \\in R^F\\) is an action at time t, and f is some function of the real world transition dynamics.\nThe objective is to find a policy \\(\\pi(x) = u\\) that minimize the expectation of a cost function \\(c(x_t)\\) over all time steps:\n\nThe policy is found by iterating between learning the posterior of the world transition dynamics and simulating using the dynamics and optimizing the policy:\n\nThe original paper uses a gaussian process to model \\(f(x_t)\\), which gives an analytical solution for the posterior. However, gaussian processes does not scale well for large datasets, and the Deep PILCO paper instead uses a neural net to estimate the same dynamics. They approximate the posterior using variational inference and minimize the KL-divergence through using dropout, which can be interpreted as a variational bayesian approximation.\nBoth papers show that their algorithm is more data efficient than current state of the art reinforcement learning algorithms:"
  },
  {
    "objectID": "posts/2018-08-19-kdd-workshop.html",
    "href": "posts/2018-08-19-kdd-workshop.html",
    "title": "KDD workshop Deep learning Day: Five lessons from building a deep neural network recommender for marketplaces",
    "section": "",
    "text": "Ning Zhou, Audun Øygard and I got a paper in the KDD workshop Deep Learning Day. We provide some practitioner’s findings on applying deep learning recommendations in production! Link to paper here.\n\n\nTogether with @nzhou9 and @matsiyatzy, I am officially moving into academia after being an industrial observer: We got a paper in the #KDD2018 workshop Deep Learning Day. We provide some practitioner's findings on applying deep learning recommendations in production!\n\n— Simen Eide (@simeneide) August 18, 2018\n\n\n\n\nThat was fun. pic.twitter.com/KBLjYlvzS9\n\n— Simen Eide (@simeneide) August 20, 2018\n\n\nPoster:"
  },
  {
    "objectID": "posts/2017-07-30-jupyter-lab.html",
    "href": "posts/2017-07-30-jupyter-lab.html",
    "title": "Jupyter lab - First impression",
    "section": "",
    "text": "Every three months or so I get really annoyed about Jupyter Notebook being so limited, and I usually spend half a day browsing alternatives like Spyder, PyCharm and Rodeo. Usually my search phrase is “Rstudio for python”, but wasting half a day or more I still end up with jupyter notebook. Although many good alternatives, the fact that you can work in the browser directly on the server makes it very simple to set up.\nThe last two weeks I have been testing out jupyter lab as a substitute for jupyter notebook for development work. Jupyter lab comes from the jupyter team, and is currently in their “Very early developer preview Alpha”, whatever that means. I have mainly used the notebook part of it, and that works more or less the same as jupyter notebook. Except for changing the locations of some buttons, jupyter lab does not (even in very early preview alpha) limit the use for a normal notebook user.\nHowever, they have taken it much closer to an IDE by including tabs, window locations, a shell and a text editor. By doing that, it is actually possible to develop in a .py file and simultaneously run the code in a console. This is much closer to what my beloved Rstudio does for R, and is highly appreciated.\nThe main thing I feel is missing is all the keyboard shortcuts. - A Ctrl+Enter to execute a selected code block in the .py file onto the console is my main loss. - Also, I find myself reorganising the tabs all the time. Shortcuts to arrange the different tabs in different ways like ShiftIt would speed up development.\nOf course, I do not hold grudge against the team since they are only in its early preview developer alpha stage. I look forward to the time they move from “very early developer preview alpha” to just “early developer preview alpha”!"
  },
  {
    "objectID": "posts/2018-11-23-bpmf-paper.html",
    "href": "posts/2018-11-23-bpmf-paper.html",
    "title": "BPMF presentation",
    "section": "",
    "text": "Slides from my presentation on “Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo” by Salakhutdinov and Mnih. Paper link."
  },
  {
    "objectID": "posts/2018-01-03-talks-and-projects-2017.html",
    "href": "posts/2018-01-03-talks-and-projects-2017.html",
    "title": "Deep Recommenders, Car Pricing, Self driving rc-car and other projects in 2017",
    "section": "",
    "text": "I am not a fan of new years resolutions. If I had been, one of my new years resolutions would be to be better at writing down what I am doing all the time. However, I held some talks and did some fun experiments in 2017, so the cheap way is simply to link to those.\n\nHow to become a Data Scientist in 20 minutes (JavaZone 2017)\nAt Javazone 2017 I held a short talk on how building (drum roll) machine learning algorithms is actually pretty easy. Simply put: I build a regression model that would predict the fair price of a car, and then I explained how I used that model to actually buy the car. The main message was that as long as you do your model validation properly (dont train and test on same data), you dont really need to understand the algorithm to get good results (use random forest!).\nWe published both algorithm and the dataset here so that anyone could replicate it, and JavaZone even filmed it:\n\n\n\n\nAlgorithms we are using in FINN.no to recommend you good stuff\n\nI have actually held three talks on this. First off was the one I held at Oslo Data Science meetup in February, where I talked about the journey we did from pure CF-models to using Tensorflow.\nAfter that we have tried out a lot of things, and also moved to use keras instead, so we could skip all the boilerplate. Ive held two talks this fall on it: one beer-talk hosted by Bekk Consulting, and one early-morning talk with Bearpoint. The presentations were very similar, and can be downloaded here.\n\n\nTesting RNN #recommenders for @FINN_tech. The RNN (bottom) generalize better when looking at last 10 items compared to only the last (mid). pic.twitter.com/feZMcz1n97\n\n— Simen Eide (@simeneide) August 31, 2017\n\n\n\n\nWorkshop for FINN developers on Machine Learning\nIn November I held a 3 hour workshop with some colleages on machine learning. The idea was, just as in the javazone talk, to demystify building these models. We spun up GPU machines to all the participants, prepared a dataset many FINN ads and a baseline script on how they could classify the ads based on their title. It was around 500’000 ads spread over 20 or so categories. The task was then to understand the baseline algorithm, then improve it by changing architechture, learning rates, optimizers and so on. The baseline started by taking averages of the word2vec vectors of the title, but the winners used a GRU-layer on the title to snitch the last percentage points on the validation accuracy. That was really impressive, none of them had done machine learning before, and then they start tinkering with recurrent neural nets!\n\n\nData scientist beware! Sixty developers at @FINN_tech with no prior #MachineLearning experience just build a deep neural classifier in 2.5hrs that beat my model! pic.twitter.com/HmG29pi7mM\n\n— Simen Eide (@simeneide) November 21, 2017\n\n\n\n\nClothing GANs\nWe had a little GAN-workshop at the end of the year here, where we among other things trained a model to generate new clothing. I was impressed how easy that was. Maybe FINN should start generating images of your ad if you cant be bothered to take your own photos? ;)\n\n\nDo you need to sell off some clothes at @FINN_tech, but not happy with your own image? This is our first try at building a #GAN model that can generate arbitrary clothing images for you! Next steps: upscaling, conditioning and maybe a fashion show? pic.twitter.com/Vkd7takdHT\n\n— Simen Eide (@simeneide) December 17, 2017\n\n\n\n\nNeural Search Engines\nIve tried to work on models that take text input and outputs a finn ad (aka search engine). They were also enhanced with user-features, so that the search would be personalized to the exact user. It worked all right, but we are currently working on a “simpler” way to personalize FINNs search results, combining the good old search engine with our recommendation models.\n\n\nSearch engines are maybe not the most sexy, but it was really fun to learn a model to predict our #recommendation vectors based on a word! pic.twitter.com/yPDsLkKYxc\n\n— Simen Eide (@simeneide) September 5, 2017\n\n\n\n\nSelf driving rc-car ++\nIve been tinkering a bit with a self driving car. The project is called donkeycar. Basically it is a rc-car that you can run through a python API with a raspberry pi. They have also integrated tensorflow, so that you can use imitation learning to drive a path. I got it to follow the road, and also a white line. Hopes was to spend enough time to build some reinforcement learning into it, but I haven’t had time (yet!).\n\n\nBil + nevralt nett = SELVKJØRENDE BIL! Kan jeg claime Norges første selvkjørende amatørbil?! #autonomousdriving #donkeycar #in #DeepLearning pic.twitter.com/vwZJcfLqLd\n\n— Simen Eide (@simeneide) August 18, 2017\n\n\n\n\nOur self driving project isnt fast, but atleast can go on forever. Driving using a single neural network on a raspberry PI #donkeycar pic.twitter.com/6y5sf8AnMi\n\n— Simen Eide (@simeneide) October 28, 2017\n\n\n\n\nHow my #donkeycar is detecting road, grass and horizon for #autonomousdriving. Slow and steady progress.. pic.twitter.com/VlL3hGmcWn\n\n— Simen Eide (@simeneide) September 1, 2017"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "eide.ai",
    "section": "",
    "text": "Dynamic Slate Recommendation with Gated Recurrent Units and Thompson Sampling\n\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Neural Nets and how to effectively train them with Stochastic Gradient Markov Chain Monte Carlo\n\n\n\n\n\nAn intuitive review of training bayesian neural nets with markov chain methods\n\n\n\n\n\n\nNov 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\nRunning Google Colab with VS code\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\nInstalling Pytorch on a raspberry pi 4\n\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\nPILCO and Deep PILCO\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2019\n\n\n\n\n\n\n  \n\n\n\n\nTakeaways Recsys 2018\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2018\n\n\n\n\n\n\n  \n\n\n\n\nBPMF presentation\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2018\n\n\n\n\n\n\n  \n\n\n\n\nKDD workshop Deep learning Day: Five lessons from building a deep neural network recommender for marketplaces\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2018\n\n\n\n\n\n\n  \n\n\n\n\nDeep Recommenders, Car Pricing, Self driving rc-car and other projects in 2017\n\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2018\n\n\n\n\n\n\n  \n\n\n\n\nDeep NLP-based Recommenders at Finn.no\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2017\n\n\n\n\n\n\n  \n\n\n\n\nJupyter lab - First impression\n\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2017\n\n\n\n\n\n\n  \n\n\n\n\nCollaborative Filtering Recommendations in Spreadsheets\n\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2017\n\n\n\n\n\n\n  \n\n\n\n\nPresentation from Oslo Data Science Meetup\n\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Math guy with hacking skills at the Norwegian marketplace FINN.no and at the machine learning startup Arctic Datalab. I am also doing an industrial phd in statistics at University of Oslo where I focus on personalization, bayesian statistics and bandits. Working on personalization systems and other machine learning tasks using behaviour, image and text. Background from mathematics, statistics and financial modeling."
  }
]